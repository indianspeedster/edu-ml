{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevent libraries"
   ],
   "id": "3b22fa4c-d6f3-447c-8a43-3fcad1e96e1a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import math\n",
    "seed=123"
   ],
   "id": "b84de416-bf1a-4afe-a7b4-6418fa51f8a4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train and test data"
   ],
   "id": "98f3fce1-52d8-4b18-98ec-9909a444c853"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ],
   "id": "166ce28d-506b-433b-9716-e66d524433d9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ],
   "id": "157a739d-d005-401e-9006-72327383c9de"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "train_data['intent']=le.fit_transform(train_data['intent'])\n",
    "test_data['intent']=le.transform(test_data['intent'])\n",
    "train_data = train_data.drop(\"Unnamed: 0\", axis=1)"
   ],
   "id": "55f0f999-ae88-4d22-a47d-aae4eb320a29"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data to train and validation"
   ],
   "id": "87e36a69-1d68-4b1b-a6a6-317b4d5125c6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,val_data=train_test_split(train_data,test_size=0.10 ,random_state=seed, shuffle=True)"
   ],
   "id": "a6fcf736-de68-4130-8da7-c1f4a4339e2b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random 30 samples from training data"
   ],
   "id": "12373a48-908e-4fe8-b80b-472ad987b6e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the unique intent\n",
    "unique_labels = df_train['intent'].unique()\n",
    "# Creating an empty dataframe to store all the values\n",
    "sampled_df = pd.DataFrame()\n",
    "#Iterating through each label and take random 30 samples from it\n",
    "for label in unique_labels:\n",
    "    label_df = df_train[df_train['intent'] == label]\n",
    "    samples = label_df.sample(n=30, random_state=seed)\n",
    "    sampled_df = sampled_df.append(samples)\n",
    "sampled_df.reset_index(drop=True, inplace=True)"
   ],
   "id": "79aeedbb-73e6-4662-a180-c6d3dfe242f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3 unique 10-shot dataset from previous sampled data"
   ],
   "id": "6e69c6ad-e85a-487e-bc89-bfa19f8d9b2f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_df\n",
    "# Create a column sample and mark it all as False and when you pick a sample mark them as True. This will make sure that you are not repeating the same sample again.\n",
    "df['sampled'] = False\n",
    "\n",
    "#creating a list to store the 10 shot dataset\n",
    "training_datasets = []\n",
    "\n",
    "for i in range(3):\n",
    "    dataset = pd.DataFrame()\n",
    "    for label in df['intent'].unique():\n",
    "        label_df = df[(df['intent'] == label) & (df['sampled'] == False)]\n",
    "        if len(label_df) >= 10:\n",
    "            samples = label_df.sample(n=10)\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "        else:\n",
    "            samples = label_df\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "    dataset = dataset.drop(\"sampled\",axis=1)\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    training_datasets.append(dataset)\n",
    "\n",
    "# The output of this cell will create a list training_datasets which contains 3 10-shot dataset"
   ],
   "id": "7ffb0f11-4abb-4fee-9513-d4763e7d036e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data"
   ],
   "id": "bbf21c0e-45e7-400a-97ad-c3e3f8272527"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('training_datasets.pkl', 'wb') as file:\n",
    "    pickle.dump(training_datasets, file)\n",
    "with open('val_data.pkl', 'wb') as file:\n",
    "    pickle.dump(val_data, file)\n",
    "with open('test_data.pkl', 'wb') as file:\n",
    "    pickle.dump(test_data, file)\n",
    "with open('train_data_full.pkl', 'wb') as file:\n",
    "    pickle.dump(train_data, file)"
   ],
   "id": "322fcc8c-4082-4e84-a83c-d9789b542bfa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Upper and lower bound of n and alpha = 0.5 & 0.75"
   ],
   "id": "e77d45ae-f641-4705-a308-874c28b2e82e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "id": "6529e76d-dd6a-490c-b54e-12f517a2c430"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation function"
   ],
   "id": "f34e56d5-b14e-4a3f-9783-398faabc3a11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(sentence, alpha=0.5 ):\n",
    "  sentence = sentence.split(\" \")\n",
    "  word_index = [i for i in range(len(sentence)) if sentence[i].lower() not in stop_words]\n",
    "  n = math.ceil(alpha*len(word_index))\n",
    "  n_random = random.sample(word_index, n)\n",
    "  for num in n_random:\n",
    "    word = sentence[num]\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "      for synonym in synset.lemmas():\n",
    "        synonyms.append(synonym.name())\n",
    "    if len(synonyms)>=2:\n",
    "      sentence[num] = synonyms[1]\n",
    "    else:\n",
    "      pass\n",
    "  return \" \".join(sentence)"
   ],
   "id": "854c8d4f-bb26-4907-aa80-c176a2c96c15"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data augmentation on each of the three training datasets"
   ],
   "id": "85f56863-9b38-4bda-b99c-4860cab104c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_datasets = []\n",
    "for train_data in training_datasets:\n",
    "  augmented_data = train_data.copy()\n",
    "  augmented_data[\"speech_text\"] = augmented_data[\"speech_text\"].apply(augmentation, alpha=0.6)\n",
    "  augmented_data = pd.concat([train_data, augmented_data])\n",
    "  augmented_datasets.append(augmented_data)"
   ],
   "id": "bd705e84-1f73-438a-bf0c-581465d13eed"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the augmented_data"
   ],
   "id": "a6758ac1-0039-4640-aa5d-fb8a6d11ae61"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('augmented_datasets.pkl', 'wb') as file:\n",
    "    pickle.dump(augmented_datasets, file)"
   ],
   "id": "3c6314cb-9c43-4dcf-b702-0b8266c8ed8b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output of this Notebook\n",
    "\n",
    "This notebook will generate 4 files as mentioned below :\n",
    "\n",
    "-   training_datasets.pkl\n",
    "\n",
    "-   val_data.pkl\n",
    "\n",
    "-   test_data.pkl\n",
    "\n",
    "-   augmented_datasets.pkl"
   ],
   "id": "01ca618a-6109-4bf8-93e7-de0a3496d2ac"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
