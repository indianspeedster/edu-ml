{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation, Preprocessing and Tokenization\n",
    "\n",
    "If you are on this notebook then you have made a choice in the previous section to go ahead with floor value while calculating n.\n",
    "\n",
    "Let’s understand this data augmentation strategy in depth with an example,\n",
    "\n",
    "since alpha is a constant , we will fix alpha as 0.75\n",
    "\n",
    "example 1: “what do you know about Fringe in Edinburgh next year?”\n",
    "\n",
    "step1 : count the length of total words without stopwords. So we have l = 7\n",
    "\n",
    "step2 : calculate n using n = alpha \\* l, substituting the value of n and alpha we have n = 0.75\\*7, n = 5.25.\n",
    "\n",
    "step3 : Since we choosed to go with the floor value for n, we will take n = 5.\n",
    "\n",
    "step4 : we will pick 5 random words and then subsitute them with their synonyms.\n",
    "\n",
    "Now, what difference would it make if we pick the ceil value instead of the floor ?\n",
    "\n",
    "So, when we see our data, the speech text is short where the number of words lies in the range (4, 16) so one word can make a difference in the model, It is important note that ceil value or floor value while calculating “n” may create an impact in the final results.\n",
    "\n",
    "Let’s implement the augmentation strategy with floor value of “n” while calculating n = alpha\\*l"
   ],
   "id": "643148ce-47f3-4cf4-9244-83ecdc26d80e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevent libraries"
   ],
   "id": "3eaa8c4c-5a8a-4bcb-96c5-425a5c9e999b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "import datasets\n",
    "from tqdm import tqdm\n",
    "seed=123"
   ],
   "id": "d5a0a893-4975-40ba-9411-82080442946e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation function"
   ],
   "id": "003b18f7-950e-4b67-b929-9e94923766b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(sentence, alpha=0.75 ):\n",
    "  sentence = sentence.split(\" \")\n",
    "  word_index = [i for i in range(len(sentence)) if sentence[i].lower() not in stop_words]\n",
    "  n = int(alpha*len(word_index))\n",
    "  n_random = random.sample(word_index, n)\n",
    "  for num in n_random:\n",
    "    word = sentence[num]\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "      for synonym in synset.lemmas():\n",
    "        synonyms.append(synonym.name())\n",
    "    if len(synonyms)>=2:\n",
    "      sentence[num] = synonyms[1]\n",
    "    else:\n",
    "      pass\n",
    "  return \" \".join(sentence)"
   ],
   "id": "3c15a63b-fcec-4c8a-9d6e-9c824a0cb83b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are done with our data augmentation function, next we will work on data preprocessing, applying data augmentation strategy and tokenizing the data which will make our data ready to be trained.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "686724d5-448c-41cb-ad35-03bb054d077b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the last Notebook we obtained train.csv and test.csv file, these files contains raw data and the data preprocessing step will act as a bridge between raw data and the final result which we are focused to achieve. Data preprocessing can have a major impact on the final results so it’s crucial to thoroughly understand what the authors did and how you could follow the same strategy.\n",
    "\n",
    "From our understanding of the paper, The author did these three major steps:\n",
    "\n",
    "-   Encoding the labels: A machine learning model always needs a number as input instead of raw text data, so label encoding is a crucial step here.\n",
    "\n",
    "-   Sampling for full few shot Learning : The full few shot setup requires 10 samples for each label, so the author randomly took 10 samples from the dataset. it’s important to make sure that you are picking unique samples. Since the author is experimenting on a 3 fold data so we will pick 30 samples and make 3 fold dataset with 10 samples each.\n",
    "\n",
    "-   Data Augmentation : The next step which author followed was implementing the data augmentation strategy and then apply the same of the previously selected 10 samples for each intent. Again the augmentation will be applied on 3 fold dataset.\n",
    "\n",
    "-   Tokenization : The last step in this notebook will be to tokenize the data, since machine learning model only understands numerical form of data, so it’s necessary to make the data in the form of tokens and this process is known as tokenization.\n",
    "\n",
    "Once we are done with the above 4 steps we will store the data and make it available for use in the next part of Reproducibility.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "287711c3-12b1-4aa1-bb55-8cfea38e8f43"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the train and test data"
   ],
   "id": "a923e423-54d4-4083-a0a6-ad34bdf48b48"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ],
   "id": "460afc2e-8021-49be-96b1-1d9c80a8c898"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ],
   "id": "303022be-6bef-45c4-abbc-e99623d30628"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "train_data['intent']=le.fit_transform(train_data['intent'])\n",
    "test_data['intent']=le.transform(test_data['intent'])\n",
    "train_data = train_data.drop(\"Unnamed: 0\", axis=1)"
   ],
   "id": "d4ab771c-28fa-491c-9c18-a6e5e6e2ef73"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data to train and validation\n",
    "\n",
    "we are again spliting the data into train and validation as when we train our model the best model while training will be selected on the basis of the validation data accuracy. ie(The model will be considered as the best model on a specific epoch when that epoch has the highest validation accuraccy.)"
   ],
   "id": "8c8373f4-015f-4acf-a0ba-1e59cdd60c17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,val_data=train_test_split(train_data,test_size=0.10 ,random_state=seed, shuffle=True)"
   ],
   "id": "9c651e76-92b1-4aa5-b947-8a882293077b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get random 30 samples from training data"
   ],
   "id": "43d82ea4-daf7-4004-b84c-2e96fe24ef76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the unique intent\n",
    "unique_labels = df_train['intent'].unique()\n",
    "# Creating an empty dataframe to store all the values\n",
    "sampled_df = pd.DataFrame()\n",
    "#Iterating through each label and take random 30 samples from it\n",
    "for label in unique_labels:\n",
    "    label_df = df_train[df_train['intent'] == label]\n",
    "    samples = label_df.sample(n=30, random_state=seed)\n",
    "    sampled_df = sampled_df.append(samples)\n",
    "sampled_df.reset_index(drop=True, inplace=True)"
   ],
   "id": "669d307a-237a-421c-a7d7-1a52d1704140"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create 3 unique 10-shot dataset from previous sampled data"
   ],
   "id": "f6959f96-81a2-4832-853f-d301833f1f82"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_df\n",
    "# Create a column sample and mark it all as False and when you pick a sample mark them as True. This will make sure that you are not repeating the same sample again.\n",
    "df['sampled'] = False\n",
    "\n",
    "#creating a list to store the 10 shot dataset\n",
    "training_datasets = []\n",
    "\n",
    "for i in range(3):\n",
    "    dataset = pd.DataFrame()\n",
    "    for label in df['intent'].unique():\n",
    "        label_df = df[(df['intent'] == label) & (df['sampled'] == False)]\n",
    "        if len(label_df) >= 10:\n",
    "            samples = label_df.sample(n=10)\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "        else:\n",
    "            samples = label_df\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "    dataset = dataset.drop(\"sampled\",axis=1)\n",
    "    dataset = dataset.reset_index(drop=True)\n",
    "    training_datasets.append(dataset)\n",
    "\n",
    "# The output of this cell will create a list training_datasets which contains 3 10-shot dataset"
   ],
   "id": "7f34bc09-5d38-4eb6-b777-11bf7bcd2877"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full = train_data"
   ],
   "id": "5803f7d4-ef1f-41fe-ac51-b484b9140fba"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentating data"
   ],
   "id": "182d465d-0751-49f0-b882-985b6e128b69"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading stop words\n",
    "stop_words = set(stopwords.words('english'))"
   ],
   "id": "9f636a43-6bb9-47ec-8b99-3d818b5b0dda"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply data augmentation on each of the three training datasets"
   ],
   "id": "86081a68-b74d-469f-9b62-f31929221a4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_datasets = []\n",
    "for train_data in training_datasets:\n",
    "  augmented_data = train_data.copy()\n",
    "  augmented_data[\"speech_text\"] = augmented_data[\"speech_text\"].apply(augmentation, alpha=0.6)\n",
    "  augmented_data = pd.concat([train_data, augmented_data])\n",
    "  augmented_datasets.append(augmented_data)"
   ],
   "id": "1ee420fc-1a91-4d06-93a8-daa4ce6ad846"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is a fundamental process in natural language processing that plays an important role in results that any of the language model produces. All the major language models have their specific tokenizer. Since the author of the paper used Bert Large Uncased so for our reproducibility process by default we have only one choice of tokenizer. Below are some of the specific task that Bert Large tokenizer will perform:\n",
    "\n",
    "-   Text Segmentation\n",
    "\n",
    "-   Vocabulary Mapping\n",
    "\n",
    "-   Subword Tokenization\n",
    "\n",
    "-   Special Tokens\n",
    "\n",
    "In the next section of this notebook we will be implementing the tokenization step.\n",
    "\n",
    ":::"
   ],
   "id": "b69aae17-fea9-4876-95de-6664b7e1928b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up tokenizer"
   ],
   "id": "ad2e6f49-e613-4bd4-986d-670ce55fa570"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_BERTmodel='bert-large-uncased'\n",
    "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
   ],
   "id": "f3ddf5d2-a054-4675-b5cf-750509bf3d94"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have below mentioned 5 dataset to tokenize\n",
    "\n",
    "-   **training_datasets** : This contains a list which has 3 full few shot data of 10 samples each.\n",
    "\n",
    "-   **val_data** : This contains data in the form of a pandas dataframe which will be used as validation data while training the model.\n",
    "\n",
    "-   **test_data** : This contains data in the form of a pandas dataframe whille be used as test data for the model.\n",
    "\n",
    "-   **augmented_datasets** : This contains a list of 3 pandas datafram where each dataframe has 20 samples which contains 10 original and 10 augmented version of the original data.\n",
    "\n",
    "-   **train_data_full** : This contains a pandas data frame with entire training data."
   ],
   "id": "ff31ef67-436e-4cb7-96f6-0c2437d1d07a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to tokenize the data"
   ],
   "id": "3e47ad07-816c-4057-920a-063c0e0fae64"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(example):\n",
    "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
    "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
   ],
   "id": "b7edc8b8-54e0-44c5-b8bb-5386056df0f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing non augmented training data"
   ],
   "id": "43173974-9682-478a-80bc-15c3eb8610c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=[]\n",
    "for train_data_ in training_datasets:\n",
    "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
    "  train_dataset.append(traindataset.map(tokenize_data))"
   ],
   "id": "7875008a-2894-44f3-80b3-a2c05c53a933"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing augmented training data"
   ],
   "id": "a63b334c-c1b0-4d86-963d-abc2bf5c2b51"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_train_dataset=[]\n",
    "for train_data_ in augmented_datasets:\n",
    "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
    "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
   ],
   "id": "05843c88-34dd-4f8d-9a90-6b24ed7c5c24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing validation data"
   ],
   "id": "4df73543-4392-4ddc-9241-4b7c12d9363a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data = datasets.Dataset.from_pandas(val_data)\n",
    "val_data = val_data.map(tokenize_data)"
   ],
   "id": "05a995d1-031e-4861-a986-0ef3dfbad111"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing test data"
   ],
   "id": "f5b1452d-1400-4556-bd44-7106d38abd19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset = datasets.Dataset.from_pandas(test_data)\n",
    "test_dataset = testdataset.map(tokenize_data)"
   ],
   "id": "2257f87d-2a10-4b79-ad5a-1dcfb69bf131"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize full train dataset"
   ],
   "id": "532a1fd8-6f68-4930-8dc1-63ffb3d2967a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_full = datasets.Dataset.from_pandas(train_data_full)\n",
    "train_data_full = train_data_full.map(tokenize_data)"
   ],
   "id": "08f9a670-4e44-4d95-9e75-562c0c8a2206"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store tokenized data"
   ],
   "id": "e30078fb-9deb-4ecb-86be-b0445aa93d9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
    "    pickle.dump(train_dataset, file)\n",
    "with open('val_data_tokenized.pkl', 'wb') as file:\n",
    "    pickle.dump(val_data, file)\n",
    "with open('test_data_tokenized.pkl', 'wb') as file:\n",
    "    pickle.dump(test_dataset, file)\n",
    "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
    "    pickle.dump(augmented_train_dataset, file)\n",
    "with open('train_dataset_full_tokenized.pkl', 'wb') as file:\n",
    "    pickle.dump(list(train_data_full), file)"
   ],
   "id": "235d7ee9-55e5-442b-8123-b32ef329bcae"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output of this Notebook\n",
    "\n",
    "This notebook will generate 4 files as mentioned below :\n",
    "\n",
    "-   training_datasets.pkl\n",
    "\n",
    "-   val_data.pkl\n",
    "\n",
    "-   test_data.pkl\n",
    "\n",
    "-   augmented_datasets.pkl"
   ],
   "id": "d7abe60f-c8c3-4eb6-91e6-536f0ac7370e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During our data preprocessing steps we made sure the three things cleaning, transforming, and organizing data before it’s fed into a model. It’s important to follow the exact same preprocessing pipeline to ensure that the data is consistent and prepared in the same way as in the original study. If we have made a wrong assumption then it would lead to a different outcome and inaccurate results.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "025845f0-626c-46e9-9c66-111ad4ffffb1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now we are done with data preprocessing, data augmentation and tokenization. Now our data is ready to be fed to the model for training. But befor that we need to set up the training parameters and hyper parameters for our model.\n",
    "\n",
    "When we see the paper, the paper talked about the hyperparameters such as epochs and batch size but for rest they said they used standard hyperparameters for Bert Large models. But when we researched we found that there is no specific optimizer for BERT large models, it depends upon task. so for our Classification task we were left with many choices but when it comes to picking two standard optimizer for this task we are left with two below mentioned options:\n",
    "\n",
    "-   [SGD (Stochastic Gradient Descent)](/): It is an optimization algorithm used to train machine learning models by iteratively adjusting model parameters using randomly selected mini-batches of data, with the goal of minimizing the loss function.\n",
    "\n",
    "-   [Adamw](/) : AdamW optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments with an added method to decay weights per the techniques discussed in the paper, ‘Decoupled Weight Decay Regularization’ by Loshchilov, Hutter et al., 2019.\n",
    "\n",
    "Considering the above mentioned 2 choices we have created two notebooks, The first notebook uses AdamW and the second notebook uses SGD. You can pick your own choice and see what are the results :\n",
    "\n",
    "-   [Notebook(Tokenization + Adamw as optimizer)](/4_train_args_1.ipynb)\n",
    "\n",
    "-   [Notebook(Tokenization + SGD as optimizer)](/4_train_args_2.ipynb)\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "12f4519d-46c3-4361-830e-386aa8e227cf"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
