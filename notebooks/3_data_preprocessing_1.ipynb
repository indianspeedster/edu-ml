{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f3f8bc-d3f4-41cb-826c-1dbb715e63d6"
      },
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "From the last Notebook we obtained train.csv and test.csv file, these files contains raw data and the data preprocessing step will act as a bridge between raw data and the final result which we are focused to achieve. Data preprocessing can have a major impact on the final results so it’s crucial to thoroughly understand what the authors did and how you could follow the same strategy.\n",
        "\n",
        "From our understanding of the paper, The author did these three major steps:\n",
        "\n",
        "-   Encoding the labels: A machine learning model always needs a number as input instead of raw text data, so label encoding is a crucial step here.\n",
        "\n",
        "-   Sampling for full few shot Learning : The full few shot setup requires 10 samples for each label, so the author randomly took 10 samples from the dataset. it’s important to make sure that you are picking unique samples. Since the author is experimenting on a 3 fold data so we will pick 30 samples and make 3 fold dataset with 10 samples each.\n",
        "\n",
        "-   Data Augmentation : The next step which author followed was implementing the data augmentation strategy and then apply the same of the previously selected 10 samples for each intent. Again the augmentation will be applied on 3 fold dataset.\n",
        "\n",
        "Once we are done with the above three steps we will store the data and make it available for use in the next part of Reproducibility.\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "01f3f8bc-d3f4-41cb-826c-1dbb715e63d6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "650e85cb-7735-4a1a-b4d5-8d649488b5bd"
      },
      "source": [
        "### Importing relevent libraries"
      ],
      "id": "650e85cb-7735-4a1a-b4d5-8d649488b5bd"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cc4ac499-5b9c-45a6-a8b7-75173da911af",
        "outputId": "5c79d517-b7ee-4906-b5ed-bedc684f6843",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/cc/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /home/cc/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "seed=123"
      ],
      "id": "cc4ac499-5b9c-45a6-a8b7-75173da911af"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c24d2640-e83d-4093-8c5e-492b4c5870dc"
      },
      "source": [
        "### Loading the train and test data"
      ],
      "id": "c24d2640-e83d-4093-8c5e-492b4c5870dc"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "19b8162d-244a-4bbe-8418-c1282ae819d0"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")"
      ],
      "id": "19b8162d-244a-4bbe-8418-c1282ae819d0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71d5466d-fd20-40d8-a8b6-f978178bb60c"
      },
      "source": [
        "### Encode the labels"
      ],
      "id": "71d5466d-fd20-40d8-a8b6-f978178bb60c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dab0b8e2-cc6f-4480-b337-935365be7bd0"
      },
      "outputs": [],
      "source": [
        "le=LabelEncoder()\n",
        "train_data['intent']=le.fit_transform(train_data['intent'])\n",
        "test_data['intent']=le.transform(test_data['intent'])\n",
        "train_data = train_data.drop(\"Unnamed: 0\", axis=1)"
      ],
      "id": "dab0b8e2-cc6f-4480-b337-935365be7bd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47167c2c-dd85-4bdd-b6fd-687f2e7b1a75"
      },
      "source": [
        "### Split the training data to train and validation\n",
        "\n",
        "we are again spliting the data into train and validation as when we train our model the best model while training will be selected on the basis of the validation data accuracy. ie(The model will be considered as the best model on a specific epoch when that epoch has the highest validation accuraccy.)"
      ],
      "id": "47167c2c-dd85-4bdd-b6fd-687f2e7b1a75"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "0cb6a774-49a0-4cfd-830b-08232e79dc9f"
      },
      "outputs": [],
      "source": [
        "df_train,val_data=train_test_split(train_data,test_size=0.10 ,random_state=seed, shuffle=True)"
      ],
      "id": "0cb6a774-49a0-4cfd-830b-08232e79dc9f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db30adb1-9759-490e-a096-83cf54843a25"
      },
      "source": [
        "### Get random 30 samples from training data"
      ],
      "id": "db30adb1-9759-490e-a096-83cf54843a25"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5096292b-769c-49f8-a95e-e9768404af76"
      },
      "outputs": [],
      "source": [
        "# Getting the unique intent\n",
        "unique_labels = df_train['intent'].unique()\n",
        "# Creating an empty dataframe to store all the values\n",
        "sampled_df = pd.DataFrame()\n",
        "#Iterating through each label and take random 30 samples from it\n",
        "for label in unique_labels:\n",
        "    label_df = df_train[df_train['intent'] == label]\n",
        "    samples = label_df.sample(n=30, random_state=seed)\n",
        "    sampled_df = sampled_df.append(samples)\n",
        "sampled_df.reset_index(drop=True, inplace=True)"
      ],
      "id": "5096292b-769c-49f8-a95e-e9768404af76"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "542e79cb-f249-4dd0-aca7-937530003c85"
      },
      "source": [
        "### create 3 unique 10-shot dataset from previous sampled data"
      ],
      "id": "542e79cb-f249-4dd0-aca7-937530003c85"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dbf67b29-953f-4fb6-ab18-1631ee003e65"
      },
      "outputs": [],
      "source": [
        "df = sampled_df\n",
        "# Create a column sample and mark it all as False and when you pick a sample mark them as True. This will make sure that you are not repeating the same sample again.\n",
        "df['sampled'] = False\n",
        "\n",
        "#creating a list to store the 10 shot dataset\n",
        "training_datasets = []\n",
        "\n",
        "for i in range(3):\n",
        "    dataset = pd.DataFrame()\n",
        "    for label in df['intent'].unique():\n",
        "        label_df = df[(df['intent'] == label) & (df['sampled'] == False)]\n",
        "        if len(label_df) >= 10:\n",
        "            samples = label_df.sample(n=10)\n",
        "            df.loc[samples.index, 'sampled'] = True\n",
        "            dataset = pd.concat([dataset, samples])\n",
        "        else:\n",
        "            samples = label_df\n",
        "            df.loc[samples.index, 'sampled'] = True\n",
        "            dataset = pd.concat([dataset, samples])\n",
        "    dataset = dataset.drop(\"sampled\",axis=1)\n",
        "    dataset = dataset.reset_index(drop=True)\n",
        "    training_datasets.append(dataset)\n",
        "\n",
        "# The output of this cell will create a list training_datasets which contains 3 10-shot dataset"
      ],
      "id": "dbf67b29-953f-4fb6-ab18-1631ee003e65"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcbaf6a1-c3ff-43d7-bfee-6b6beb9a4cd1"
      },
      "source": [
        "### Store data"
      ],
      "id": "bcbaf6a1-c3ff-43d7-bfee-6b6beb9a4cd1"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e26449b7-edcc-4f1e-beff-a09759560695"
      },
      "outputs": [],
      "source": [
        "with open('training_datasets.pkl', 'wb') as file:\n",
        "    pickle.dump(training_datasets, file)\n",
        "with open('val_data.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data.pkl', 'wb') as file:\n",
        "    pickle.dump(test_data, file)\n",
        "with open('train_data_full.pkl', 'wb') as file:\n",
        "    pickle.dump(train_data, file)"
      ],
      "id": "e26449b7-edcc-4f1e-beff-a09759560695"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f630b2a4-46cc-4199-aa74-09fd3edb87a2"
      },
      "source": [
        "### Data Augmentation"
      ],
      "id": "f630b2a4-46cc-4199-aa74-09fd3edb87a2"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "233efe1e-9bb7-4772-baf3-28e53341ddc5"
      },
      "outputs": [],
      "source": [
        "#loading stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "id": "233efe1e-9bb7-4772-baf3-28e53341ddc5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46ef50e9-fb79-4047-81e5-0422f6ae56a0"
      },
      "source": [
        "#### Data Augmentation function"
      ],
      "id": "46ef50e9-fb79-4047-81e5-0422f6ae56a0"
    },
    {
      "cell_type": "code",
      "source": [
        "def synonym_replacement(words, n):\n",
        "\tnew_words = words.copy()\n",
        "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
        "\trandom.shuffle(random_word_list)\n",
        "\tnum_replaced = 0\n",
        "\tfor random_word in random_word_list:\n",
        "\t\tsynonyms = get_synonyms(random_word)\n",
        "\t\tif len(synonyms) >= 1:\n",
        "\t\t\tsynonym = random.choice(list(synonyms))\n",
        "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
        "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
        "\t\t\tnum_replaced += 1\n",
        "\t\tif num_replaced >= n: #only replace up to n words\n",
        "\t\t\tbreak\n",
        "\n",
        "\t#this is stupid but we need it, trust me\n",
        "\tsentence = ' '.join(new_words)\n",
        "\tnew_words = sentence.split(' ')\n",
        "\n",
        "\treturn new_words\n",
        "\n",
        "def get_synonyms(word):\n",
        "\tsynonyms = set()\n",
        "\tfor syn in wordnet.synsets(word):\n",
        "\t\tfor l in syn.lemmas():\n",
        "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
        "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
        "\t\t\tsynonyms.add(synonym)\n",
        "\tif word in synonyms:\n",
        "\t\tsynonyms.remove(word)\n",
        "\treturn list(synonyms)"
      ],
      "metadata": {
        "id": "pMbLRV1-0w77"
      },
      "id": "pMbLRV1-0w77",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d62d1ebc-944a-41a6-beea-b9eb7043bd4e"
      },
      "outputs": [],
      "source": [
        "def augmentation(sentence, alpha=0.75 ):\n",
        "  words = sentence.split(\" \")\n",
        "  n = alpha*len(words)\n",
        "  sentence = synonym_replacement(words, n)\n",
        "\n",
        "  return \" \".join(sentence)"
      ],
      "id": "d62d1ebc-944a-41a6-beea-b9eb7043bd4e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00a7a30b-2eb3-422e-bfbc-a07220730a88"
      },
      "source": [
        "### Apply data augmentation on each of the three training datasets"
      ],
      "id": "00a7a30b-2eb3-422e-bfbc-a07220730a88"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "855e6200-6c41-487e-a4fc-24bf90a856fc"
      },
      "outputs": [],
      "source": [
        "augmented_datasets = []\n",
        "for train_data in training_datasets:\n",
        "  augmented_data = train_data.copy()\n",
        "  augmented_data[\"speech_text\"] = augmented_data[\"speech_text\"].apply(augmentation, alpha=0.75)\n",
        "  augmented_data = pd.concat([train_data, augmented_data])\n",
        "  augmented_datasets.append(augmented_data)"
      ],
      "id": "855e6200-6c41-487e-a4fc-24bf90a856fc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d1c820f-a934-415a-9520-c977bb65a001"
      },
      "source": [
        "### Store the augmented_data"
      ],
      "id": "6d1c820f-a934-415a-9520-c977bb65a001"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "a110b07d-a3b2-4de4-b08e-f530f3a2b40e"
      },
      "outputs": [],
      "source": [
        "with open('augmented_datasets.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_datasets, file)"
      ],
      "id": "a110b07d-a3b2-4de4-b08e-f530f3a2b40e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6d0147a-f1b7-4634-9bbe-590704c2dbb8"
      },
      "source": [
        "## Output of this Notebook\n",
        "\n",
        "This notebook will generate 4 files as mentioned below :\n",
        "\n",
        "-   training_datasets.pkl\n",
        "\n",
        "-   val_data.pkl\n",
        "\n",
        "-   test_data.pkl\n",
        "\n",
        "-   augmented_datasets.pkl"
      ],
      "id": "e6d0147a-f1b7-4634-9bbe-590704c2dbb8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd3a261c-0fef-4ba9-aa9a-7ffc6ebcb975"
      },
      "source": [
        "During our data preprocessing step we made sure the three things cleaning, transforming, and organizing data before it’s fed into a model. It’s important to follow the exact same preprocessing pipeline to ensure that the data is consistent and prepared in the same way as in the original study. If we have made a wrong assumption then it would lead to a different outcome and inaccurate results.\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "dd3a261c-0fef-4ba9-aa9a-7ffc6ebcb975"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e77c5287-8d6d-4e9f-9ae0-7debe0d6373e"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "Now we are done with the initial preprocessing of data and we are left with other preprocessing which is model specific so there is a seperate ntebook for the same. In the next notebook we will focus on tokenization and setting up training arguments.\n",
        "\n",
        "When we see the paper, the paper talked about the hyperparameters such as epochs and batch size but for rest they said they used standard hyperparameters for Bert Large models. But when we researched we found that there is no specific optimizer for BERT large models, it depends upon task. so for our Classification task we were left with two choices, Either use AdamW or SGD. So we have created two notebooks, The first notebook uses AdamW and the second notebook uses SGD. You can pick your own choice and see what are the results :\n",
        "\n",
        "-   [Notebook(Tokenization + Adamw as optimizer)](/)\n",
        "\n",
        "-   [Notebook(Tokenization + SGD as optimizer)](/)\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "e77c5287-8d6d-4e9f-9ae0-7debe0d6373e"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}