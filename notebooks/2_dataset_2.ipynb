{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "If you are on this notebook then you have made a choice in the previous section to go ahead with **Collected-Original-Data**. Your intuition might have said that since Bert large uncased is a case unsensitive model and annotating the data might not help to make a model better.\n",
    "\n",
    "This directory contains collected original data with normalization for numbers/date etc which contain the pre-designed human-robot interaction questions and the user answers. Entire data is in CSV format and is stored\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "46bcd947-50c3-4f7e-a4ae-08bbb1a9c089"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ],
   "id": "53b6cc97-3990-4d3f-a8c3-9e18520928c3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 123"
   ],
   "id": "368c871b-a5c3-49c3-af19-1dd5a5cb4e6f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and storing the data"
   ],
   "id": "fcd37dad-afd8-4e16-a53a-b04978fada11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data/master/Collected-Original-Data/paraphrases_and_intents_26k_normalised_all.csv\"\n",
    "dataframe = pd.read_csv(url, delimiter=\";\")"
   ],
   "id": "c15f1805-73df-4c3b-81c7-c520fed26965"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the relevent columns"
   ],
   "id": "35b7731a-c3d3-48c9-a070-3e87d974d4cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"merged\"] = dataframe[\"scenario\"] + \"_\" + dataframe[\"intent\"]\n",
    "new_df = dataframe[[\"answer\", \"merged\"]]\n",
    "new_df.columns = [\"speech_text\",\"intent\"]"
   ],
   "id": "7e81ea39-c2f2-47eb-89ef-e305237864f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(new_df, test_size=0.10, random_state = seed)"
   ],
   "id": "e5bf43ef-e9b5-436e-81df-0e29286a8d89"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting exact number of samples as mentioned in the paper"
   ],
   "id": "9dfd7797-b536-4e9c-9f87-d072fcc44f0b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(9960, random_state=seed)\n",
    "test = test.sample(1076, random_state=seed)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ],
   "id": "5bc5b19f-bd98-47c5-a42f-e30ceaa5fd52"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the data to csv"
   ],
   "id": "6dc55f41-5bb9-464f-98a5-00dcb13934e8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\")\n",
    "test.to_csv(\"test.csv\")"
   ],
   "id": "b50d5e72-4347-4288-8353-be5efd6f0601"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will produce two csv files as output.\n",
    "\n",
    "-   train.csv\n",
    "\n",
    "-   test.csv\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "3bc0c848-5987-4c9f-b6ca-422660e35407"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data ready, then we can now focus on Augmenting the data.\n",
    "\n",
    "As discussed in the paper, the Authors are following synonym replacement strategy with a special formulae n = α \\* l where n is the number of words that is going to be replaced, α is a constant whose value lies between 0 and 1 and l is the length of the sentence."
   ],
   "id": "4aebcaa2-1dc1-46f3-8f4f-33957bca6520"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s understand how this synonym replacement works.\n",
    "\n",
    "Suppose we have a sentence “is there an alarm for ten am” Step 1 is to remove the stopwords from the sentence, so now the sentence would be “there alarm ten am”.\n",
    "\n",
    "The length of the sentence now is 4 which is l.\n",
    "\n",
    "Let’s take alpha as 0.6, So, now when we perform calculation we get n = 0.75\\*4, which is equal to 3, So now we will pick three random words and replace then with their synonyms."
   ],
   "id": "ddb16d65-517d-44b8-8b3a-63f26c6ea8f3"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when calculating n, there is high probability that the value will be a decimal value and since n can be only an integer, the author never specified that which value of n we are supposed to pick. ie (ceil or floor). Intent classification task has less number of words as input and even if there is difference of one word in the the augmented text due to this ceil, floor confusion, then it may lead to different results.\n",
    "\n",
    "For the data preprocessing we have two notebooks which will focus on both the scenarios taking a ceil value for n and taking a floor value for n.\n",
    "\n",
    "-   [Notebook(DataPreProcess_floor(n))](/3_data_preprocessing_1.ipynb)\n",
    "\n",
    "-   [Notebook(DataPreProcess_ceil(n))](/3_data_preprocessing_2.ipynb)"
   ],
   "id": "4380a37c-165a-42c8-b346-90b6e10c51bd"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
