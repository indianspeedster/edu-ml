{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data\n",
    "\n",
    "In the world of machine learning, the data you use is like the building blocks of your model. Good data can help your model perform really well and give useful results, while not-so-good data can lead to results that don’t really make sense or aren’t very accurate. So, having good, reliable data is super important because it’s what your model learns from and it can make a big difference in how well your model works and the results it gives you.\n",
    "\n",
    "Likewise, when it comes to **reproducing** any kind of result, the role of data is incredibly crucial. If you’re not using the exact same data as the original study, it becomes quite challenging to achieve precisely the same outcome. Data forms the foundation for outcomes, and even small differences in the data used can lead to variations in results. Therefore, the accuracy and similarity of data used for reproduction play a vital role in ensuring the consistency and reliability of the outcomes.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "a2a0c9c7-9c1d-4acb-a41f-1710f0a4845e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ],
   "id": "746de80e-a39a-413e-97e1-21f195d63e29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "seed = 123"
   ],
   "id": "8afd32d5-9d9d-4d34-9deb-52c68fcc1fa1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching and storing the data"
   ],
   "id": "bbe78117-26da-4a72-b3f9-2a832b6a37e3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data/master/Collected-Original-Data/paraphrases_and_intents_26k_normalised_all.csv\"\n",
    "dataframe = pd.read_csv(url, delimiter=\";\")"
   ],
   "id": "715edda0-9913-4b0e-9f09-ca2af276e7c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the relevent columns"
   ],
   "id": "41ee2693-c5dd-4d3a-b1ed-db69ebd63702"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[\"merged\"] = dataframe[\"scenario\"] + \"_\" + dataframe[\"intent\"]\n",
    "new_df = dataframe[[\"answer\", \"merged\"]]\n",
    "new_df.columns = [\"speech_text\",\"intent\"]"
   ],
   "id": "06c65714-ceec-46d3-9233-64f7bb54f4d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(new_df, test_size=0.10, random_state = seed)"
   ],
   "id": "f2810bd6-9b4d-41a4-8465-6e86d95d7baa"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting exact number of samples as mentioned in the paper"
   ],
   "id": "fe1f1943-97e6-4a80-b25e-c51a773fd2e5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.sample(9960, random_state=seed)\n",
    "test = test.sample(1076, random_state=seed)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)"
   ],
   "id": "0faa49d7-5a7e-442e-b619-713beb9eeaa5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting the data to csv"
   ],
   "id": "1bdd296e-9c5d-46d2-a2d3-89e1903c1f2e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\")\n",
    "test.to_csv(\"test.csv\")"
   ],
   "id": "22eb115f-11cf-4e49-adcb-3134e916ccff"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will produce two csv files as output.\n",
    "\n",
    "-   train.csv\n",
    "\n",
    "-   test.csv\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "61ad963c-55bb-4a9f-b1b3-a6fb497a89e1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data ready, then we can now focus on Augmenting the data.\n",
    "\n",
    "As discussed in the paper, the Authors are following synonym replacement strategy with a special formulae n = α \\* l where n is the number of words that is going to be replaced, α is a constant whose value lies between 0 and 1 and l is the length of the sentence.\n",
    "\n",
    "Now when calculating n, there is high probability that the value will be a decimal value and since n can be only an integer, the author never specified that which value of n we are supposed to pick. ie (ceil or floor). Intent classification task has less number of words as input and even if there is difference of one word in the the augmented text due to this ceil, floor confusion, then it may lead to different results.\n",
    "\n",
    "For the data preprocessing we have two notebooks which will focus on both the scenarios taking a ceil value for n and taking a floor value for n.\n",
    "\n",
    "-   [Notebook(DataPreProcess_floor(n))](/)\n",
    "\n",
    "-   [Notebook(DataPreProcess_ceil(n))](/)"
   ],
   "id": "3bc47058-355f-419a-8435-c16a9c918399"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
