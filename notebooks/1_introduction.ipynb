{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re: Impact of Data Augmentation on Full few shot learning for intent classification.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "de3ee374-bccb-4f36-ab8f-70f1648cd8fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The research paper **Impact of Data Augmentation on Full few shot learning for intent classification** centers around evaluating the effects of employing a particular data augmentation strategy on intent classification using full few-shot learning. The author focuses on three different machine learning models trained under three different scenarios. 1) Full data 2) Full few shot data (10 samples of each intent) 3) Full few shot data + Augmented data (20 samples of each intent). Before moving ahead let’s understand some of the terminologies which will be often used"
   ],
   "id": "3e06b67d-6f94-4b74-bc0c-a1e4bc9d48a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claims of the paper:\n",
    "\n",
    "The above paper claims that “There is a minor improvement while training full few-shot learning models with data augmentation.” To justify the claim the paper has this table in the result section.\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<th>\n",
    "Model\n",
    "</th>\n",
    "<th>\n",
    "Accuracy\n",
    "</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Full dataset\n",
    "</td>\n",
    "<td>\n",
    "92.75\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Full few-shot dataset\n",
    "</td>\n",
    "<td>\n",
    "83\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Full few-shot dataset + Augmented dataset\n",
    "</td>\n",
    "<td>\n",
    "84.5\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Further we will follow the methodology given in the paper and reproduce the results to validate the claims made in the paper."
   ],
   "id": "89d22dc5-5086-4519-a51f-fb6c484e57ef"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "Before going ahead we need to install some of the libraries which we are going to use during the course of reproducing the results of this paper.\n",
    "\n",
    "-   [transformers](https://pypi.org/project/transformers/)\n",
    "\n",
    "-   [datasets](https://pypi.org/project/datasets/)\n",
    "\n",
    "-   [accelerate](https://pypi.org/project/accelerate/)\n",
    "\n",
    "-   [nltk](https://pypi.org/project/nltk/)\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "a9eab112-cf93-4d0d-9d4e-adbf99d2118b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install nltk"
   ],
   "id": "8f19326f-1025-479e-a347-103625d47334"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ],
   "id": "7793a2e7-7f28-4101-b899-54d89fbf2187"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "\n",
    "When it comes to **reproducing** any kind of result, the role of data is incredibly crucial. If you’re not using the exact same data as the original study, it becomes quite challenging to achieve precisely the same outcome. Data forms the foundation for outcomes, and even small differences in the data used can lead to variations in results. Therefore, the accuracy and similarity of data used for reproduction play a vital role in ensuring the consistency and reliability of the outcomes.\n",
    "\n",
    "As mentioned by the author that they used [HWU64](https://github.com/xliuhw/NLU-Evaluation-Data/) dataset and the sourcw of the data is [this](https://github.com/xliuhw/NLU-Evaluation-Data/) official Github repository and it contains natural language data for human-robot interaction in home domain which was collected and annotated for evaluating NLU Services/platforms.\n",
    "\n",
    "The above github repository contains:\n",
    "\n",
    "**Collected-Original-Data (25K)**: collected original data with normalization for numbers/date etc which contain the pre-designed human-robot interaction questions and the user answers. They are organized in CSV format.\n",
    "\n",
    "**AnnotatedData (25716 Lines)**: This contains annotated data for Intents and Entities, organized in csv format. The annotated csv file has following columns: userid, answerid, scenario, intent, status, answer_annotation, notes, suggested_entities, answer_normalised, answer, question. Most of them come from the original data collection, we keep them here for monitoring of the afterwards processing.\n",
    "\n",
    "“answer” contains the original user answers.\n",
    "\n",
    "“answer_normalised” were normalised from “answer”.\n",
    "\n",
    "“notes” was used for the annotators to keep a track of changes they have made.\n",
    "\n",
    "“status” was used for annotation and post processing.\n",
    "\n",
    "“answer_annotation” contains the annotated results which is used for generating the train/test datasets, along with “scenario”, “intent” and “status”.\n",
    "\n",
    "**10-fold cross-validation** : This is 10 fold cross validation data which is formed from the AnnotatedData. This dataset was specifically designed by the authors for their specific purpose.\n",
    "\n",
    "**Annotation Guidelines** : It contains annotation guidelines which were used to annotate the data.\n",
    "\n",
    "So when we see the above folders, we are left with 3 different data folders, 3 because Annotation Guidelines does not cantains any data for intent classification.\n",
    "\n",
    "When we check the content of the remaining three folders, we will see that Annotated data and 10 fold cross validation data is the same data, it’s just that the 10 fold data contains 10 fold of the Annotated data. Here our intuition will say to go with one of the 10 fold data, the reason why we should choose the 10 fold data instead of the entire annotated dataset because the 1 fold data of the 10 fold data contains the same amount of data as mentioned in the paper.\n",
    "\n",
    "Now we are left with 2 options\n",
    "\n",
    "1.  Collected original data:\n",
    "\n",
    "-   [Collected-Original-Data](https://github.com/xliuhw/NLU-Evaluation-Data/tree/master/Collected-Original-Data)\n",
    "\n",
    "-   [CrossValidation-Data](https://github.com/xliuhw/NLU-Evaluation-Data/tree/master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation)\n",
    "\n",
    "Now let’s understand what is the difference between Annotated data and Original data through an example.\n",
    "\n",
    "Original data: Is there an alarm for 10am?\n",
    "\n",
    "Annotated data: “is there an alarm for ten am”\n",
    "\n",
    "So from the above two examples we can see that in the annotated data all the text is converted to lower case. but we know that the author is using Bert Large Uncased model and The BERT “uncased” models are designed to be case-insensitive. So it’s difficult to say that how much effect each of the data will bring.\n",
    "\n",
    "From the above 2 options it’s dificult to pick one choice, so we will leave it upon you to pick one and go ahead.\n",
    "\n",
    "Below we have 2 notebooks that have code to load data from any of all two datafolder and you can give a try to both and see through which data we are able to get equivalent results to what the author mentioned.\n",
    "\n",
    "-   [Notebook(Collected-Original-Data)](/2_dataset_2.ipynb)\n",
    "\n",
    "-   [Notebook(CrossValidation-Data)](/2_dataset_1.ipynb)"
   ],
   "id": "84ac5d75-25c6-4c24-a264-44a920777e4c"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
