{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Training parameters and Hyperparameters\n",
    "\n",
    "If you have selected this notebook then it is assumed that you have selected to go ahead with AdamW (Adam with weight decay) as the optimizer."
   ],
   "id": "2b2a03b8-905f-4ef6-8d7c-248419be27b8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters and Hyperparameters Explanation\n",
    "\n",
    "1.  **`output_dir`**: Specifies the directory where model checkpoints and training logs will be saved.\n",
    "\n",
    "2.  **`evaluation_strategy`**: Defines the strategy for evaluating the model during training.\n",
    "\n",
    "3.  **`save_strategy`**: Specifies when to save model checkpoints.\n",
    "\n",
    "4.  **`learning_rate`**: Determines the step size at which the optimizer adjusts model weights during training.\n",
    "\n",
    "5.  **`per_device_train_batch_size`**: Specifies the batch size for training data per GPU, impacting memory usage and computational efficiency.\n",
    "\n",
    "6.  **`per_device_eval_batch_size`**: Sets the batch size for evaluation data per GPU, affecting memory and computation during evaluation.\n",
    "\n",
    "7.  **`num_train_epochs`**: Indicates the total number of training epochs, which are complete passes through the training dataset.\n",
    "\n",
    "8.  **`warmup_ratio`**: Determines the ratio of warmup steps to the total number of training steps, helping the optimizer to smoothly adapt in the initial stages of training.\n",
    "\n",
    "9.  **`weight_decay`**: Introduces L2 regularization to the optimizer, helping to prevent overfitting by penalizing large model weights.\n",
    "\n",
    "10. **`load_best_model_at_end`**: Specifies whether to load the best model based on the chosen evaluation metric at the end of training.\n",
    "\n",
    "11. **`metric_for_best_model`**: Specifies the evaluation metric used to determine the best model, which is set to \"accuracy\" in this case.\n",
    "\n",
    "12. **`save_total_limit`**: Sets the maximum number of model checkpoints to keep, preventing excessive storage usage.\n",
    "\n",
    "13. **`logging_dir`**: Designates the directory where training logs, such as training progress and performance metrics, will be stored.\n",
    "\n",
    "14. **`optimizers`**: Specifies the optimizers used for model parameter updates during training, allowing for gradient-based optimization algorithms like AdamW, SGD and many more."
   ],
   "id": "7510dd46-d1dc-4efc-b9ad-75a766ef1f5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ],
   "id": "be9c3609-a2aa-4649-be01-2cb24e5870a1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing relevent Library"
   ],
   "id": "587f6c92-0934-471a-b5a3-112bbcc0a738"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n"
   ],
   "id": "88864359-fd80-43bf-9070-fdf177f13ee6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training arguments"
   ],
   "id": "62116518-3c9b-472c-bad3-8697edbdb7df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"output_dir\": \"./output\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"num_train_epochs\": 10,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"accuracy\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"logging_dir\": \"./logs\",\n",
    "    \"optimizer\": \"AdamW\"\n",
    "}\n",
    "\n"
   ],
   "id": "75274b9c-0836-4529-bd49-f415cbbee73a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the dictionary to a json file"
   ],
   "id": "6274bdb0-1901-464f-a0da-a35e27f0d3aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('args.json', 'w') as args_file:\n",
    "    json.dump(args, args_file, indent=4)\n"
   ],
   "id": "eb2ec1f9-cc99-46e1-a6b5-e3048ef90ccb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps:\n",
    "\n",
    "Now we have completed all the steps needed before training a model, The next step is Training the model and obtaining the final result. But wait, the author talked in the paper in Training and evaluation section that they did hyperparameter tuning but they never mentioned that they did it on all three models or just the largest model. Again why this doubt comes because if they are comparing then are they comparing it on the same ground or the models specific performance ?\n",
    "\n",
    "So again now we are left with two choices\n",
    "\n",
    "-   Hyperparameter tuning on the largest model\n",
    "\n",
    "-   Hyperparameter tuining on all three models\n",
    "\n",
    "Itâ€™s upto you that what you are picking, below we have two notebook each with a different choice. Pick your own adventure and see how similar are your results.\n",
    "\n",
    "-   [Notebook(Hyperparameter tuning on the largest model)](/5_model_training_1.ipynb)\n",
    "\n",
    "-   [Notebook(Hyperparameter tuining on all three models)](/5_model_training_2.ipynb)"
   ],
   "id": "18bf3b0a-06ca-424c-82ae-2f2e654ca8d2"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
