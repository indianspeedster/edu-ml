{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cebb7ef5-e16a-4a47-abc0-bb4649fcd5de"
      },
      "source": [
        "## Getting Data\n",
        "\n",
        "In the world of machine learning, the data you use is like the building blocks of your model. Good data can help your model perform really well and give useful results, while not-so-good data can lead to results that don’t really make sense or aren’t very accurate. So, having good, reliable data is super important because it’s what your model learns from and it can make a big difference in how well your model works and the results it gives you.\n",
        "\n",
        "Likewise, when it comes to **reproducing** any kind of result, the role of data is incredibly crucial. If you’re not using the exact same data as the original study, it becomes quite challenging to achieve precisely the same outcome. Data forms the foundation for outcomes, and even small differences in the data used can lead to variations in results. Therefore, the accuracy and similarity of data used for reproduction play a vital role in ensuring the consistency and reliability of the outcomes.\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "cebb7ef5-e16a-4a47-abc0-bb4649fcd5de"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82498b3f-1886-4b01-920a-d7ce7998a60a"
      },
      "source": [
        "### importing libraries"
      ],
      "id": "82498b3f-1886-4b01-920a-d7ce7998a60a"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ef479edc-fbff-43ce-bf11-04f960ee92d3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "from zipfile import ZipFile"
      ],
      "id": "ef479edc-fbff-43ce-bf11-04f960ee92d3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "228d4567-5c24-46f6-92ea-722fbef0a5f9"
      },
      "source": [
        "### Clone the Github repository"
      ],
      "id": "228d4567-5c24-46f6-92ea-722fbef0a5f9"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3eb7524c-8d80-49e2-b640-52b51b10eccf"
      },
      "outputs": [],
      "source": [
        "repository_url = 'https://github.com/xliuhw/NLU-Evaluation-Data/archive/refs/heads/master.zip'\n",
        "\n",
        "response = requests.get(repository_url)\n",
        "with open('repository.zip', 'wb') as file:\n",
        "    file.write(response.content)\n",
        "\n",
        "with ZipFile('repository.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('repository')"
      ],
      "id": "3eb7524c-8d80-49e2-b640-52b51b10eccf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96e6482a-95d3-4153-8814-d54ea1431dc7"
      },
      "source": [
        "### Arranging the relevant data\n",
        "\n",
        "There are two subfolders inside the repository (train and test) and these folder contains many csv files with name as intent.csv where intent is the different types of intents.\n",
        "\n",
        "We will be looping through all the csv files and then create a single file which would contain all the data."
      ],
      "id": "96e6482a-95d3-4153-8814-d54ea1431dc7"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8f7b4371-b548-4b39-be47-c3824eb402d2"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for folder in [\"trainset\", \"testset/csv\"]:\n",
        "  csv_files = [file for file in os.listdir(f'repository/NLU-Evaluation-Data-master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation/KFold_1/{folder}') if file.endswith('.csv')]\n",
        "  merged_df = pd.DataFrame()\n",
        "  for csv_file in csv_files:\n",
        "      file_path = f'repository/NLU-Evaluation-Data-master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation/KFold_1/{folder}' '/' + csv_file\n",
        "      df = pd.read_csv(file_path,delimiter=\";\")\n",
        "      merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
        "  data.append(merged_df)"
      ],
      "id": "8f7b4371-b548-4b39-be47-c3824eb402d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eceb46df-66a9-469d-a650-3ba6bcaa81f1"
      },
      "source": [
        "### Extracting the relevent columns and then saving the dataframes to a csv file"
      ],
      "id": "eceb46df-66a9-469d-a650-3ba6bcaa81f1"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e82f8efa-b1aa-4cc1-aaf2-8ef2bcabc34c"
      },
      "outputs": [],
      "source": [
        "for i, merged_df in enumerate(data):\n",
        "  merged_df[\"merged\"] = merged_df[\"scenario\"] + \"_\" + merged_df[\"intent\"]\n",
        "  merged_df = merged_df[[\"answer_from_user\", \"merged\"]]\n",
        "  merged_df.columns = [\"speech_text\",\"intent\"]\n",
        "  merged_df = merged_df.dropna()\n",
        "  if i == 0:\n",
        "    merged_df.to_csv('train.csv')\n",
        "  else:\n",
        "    merged_df.to_csv('test.csv')"
      ],
      "id": "e82f8efa-b1aa-4cc1-aaf2-8ef2bcabc34c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29cf824-aa56-4db4-933d-6f8a39865ef9"
      },
      "source": [
        "The above cell will produce two csv files as output.\n",
        "\n",
        "-   train.csv\n",
        "\n",
        "-   test.csv\n",
        "\n",
        "------------------------------------------------------------------------"
      ],
      "id": "f29cf824-aa56-4db4-933d-6f8a39865ef9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17f51888-6eaf-4b92-9040-f848d6ce9b98"
      },
      "source": [
        "Once we have the data ready, then we can now focus on Augmenting the data.\n",
        "\n",
        "As discussed in the paper, the Authors are following synonym replacement strategy with a special formulae n = α \\* l where n is the number of words that is going to be replaced, α is a constant whose value lies between 0 and 1 and l is the length of the sentence.\n",
        "\n",
        "Now when calculating n, there is high probability that the value will be a decimal value and since n can be only an integer, the author never specified that which value of n we are supposed to pick. ie (ceil or floor). Intent classification task has less number of words as input and even if there is difference of one word in the the augmented text due to this ceil, floor confusion, then it may lead to different results.\n",
        "\n",
        "For the data preprocessing we have two notebooks which will focus on both the scenarios taking a ceil value for n and taking a floor value for n.\n",
        "\n",
        "-   [Notebook(DataPreProcess_floor(n))](/)\n",
        "\n",
        "-   [Notebook(DataPreProcess_ceil(n))](/)"
      ],
      "id": "17f51888-6eaf-4b92-9040-f848d6ce9b98"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  }
}