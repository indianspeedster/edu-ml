{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "If you are on this notebook then you have made a choice in the previous section to go ahead with **CrossValidation-Data**. Let’s understand in more depth what is inside this dataset directory.\n",
    "\n",
    "The CrossValidation-Data directory contains these 4 folder:\n",
    "\n",
    "-   autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation\n",
    "\n",
    "-   out4ApiaiReal/Apiai_trainset_2018_03_22-13_01_25_169/CrossValidation\n",
    "\n",
    "-   out4LuisReal/Luis_trainset_2018_03_22-13_01_25_169/CrossValidation\n",
    "\n",
    "-   out4RasaReal/rasa_json_2018_03_22-13_01_25_169_80Train/CrossValidation\n",
    "\n",
    "-   out4WatsonReal/Watson_2018_03_22-13_01_25_169_trainset/CrossValidation\n",
    "\n",
    "The first directory (autoGeneFromRealAnno/:) contains generated trainset and testset from the annotated csv file. The other four subdirectories (out4ApiaiReal, out4LuisReal, out4RasaReal and out4WatsonReal) in CrossValidation/ are the converted NLU service input data for Dialogflow, LUIS, Rasa and Watson respectively. So we will only be picking the first directory for our use case.\n",
    "\n",
    "Inside the directory “autoGeneFromRealAnno” we have 10 fold data among which we wcan choose any of the fold to train but for now we can go ahead with 1st fold. Inside First fold we have train and test directory respectively and inside the repositories we have 64 csv files which contains intent data for each of the 64 intent.\n",
    "\n",
    "Next we will try to collect the data and store it in a pandas dataframe and then a csv file.\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "49ae3c53-7906-4792-9cda-fd9a40cb3d61"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing libraries"
   ],
   "id": "8e7949dd-2d6f-4cd5-8944-eaed9354fb42"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from zipfile import ZipFile"
   ],
   "id": "4a56a89d-4f3b-4ba4-b720-358a1b95c08b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the Github repository"
   ],
   "id": "22648684-c4e3-475f-8470-3767690fb1bc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_url = 'https://github.com/xliuhw/NLU-Evaluation-Data/archive/refs/heads/master.zip'\n",
    "\n",
    "response = requests.get(repository_url)\n",
    "with open('repository.zip', 'wb') as file:\n",
    "    file.write(response.content)\n",
    "\n",
    "with ZipFile('repository.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('repository')"
   ],
   "id": "d07149d5-7f42-4929-a9ba-c05dd906e241"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arranging the relevant data\n",
    "\n",
    "There are two subfolders inside the repository (train and test) and these folder contains many csv files with name as intent.csv where intent is the different types of intents.\n",
    "\n",
    "We will be looping through all the csv files and then create a single file which would contain all the data."
   ],
   "id": "cdeece4b-b680-4467-9ed0-565a55998c2c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for folder in [\"trainset\", \"testset/csv\"]:\n",
    "  csv_files = [file for file in os.listdir(f'repository/NLU-Evaluation-Data-master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation/KFold_1/{folder}') if file.endswith('.csv')]\n",
    "  merged_df = pd.DataFrame()\n",
    "  for csv_file in csv_files:\n",
    "      file_path = f'repository/NLU-Evaluation-Data-master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_22-13_01_25_169/CrossValidation/KFold_1/{folder}' '/' + csv_file\n",
    "      df = pd.read_csv(file_path,delimiter=\";\")\n",
    "      merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "  data.append(merged_df)"
   ],
   "id": "1dfafc41-a07b-4493-a10a-46cded519703"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the relevent columns and then saving the dataframes to a csv file"
   ],
   "id": "b3638522-553b-46eb-a989-e6a68301e767"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, merged_df in enumerate(data):\n",
    "  merged_df[\"merged\"] = merged_df[\"scenario\"] + \"_\" + merged_df[\"intent\"]\n",
    "  merged_df = merged_df[[\"answer_from_user\", \"merged\"]]\n",
    "  merged_df.columns = [\"speech_text\",\"intent\"]\n",
    "  if i == 0:\n",
    "    merged_df.to_csv('train.csv')\n",
    "  else:\n",
    "    merged_df.to_csv('test.csv')"
   ],
   "id": "e1a4c402-c68b-44e9-ba3b-0d40df54355a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell will produce two csv files as output.\n",
    "\n",
    "-   train.csv\n",
    "\n",
    "-   test.csv\n",
    "\n",
    "------------------------------------------------------------------------"
   ],
   "id": "7867929d-9fb7-44a7-a6bf-a60d30119247"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the data ready, then we can now focus on Augmenting the data.\n",
    "\n",
    "As discussed in the paper, the Authors are following synonym replacement strategy with a special formulae n = α \\* l where n is the number of words that is going to be replaced, α is a constant whose value lies between 0 and 1 and l is the length of the sentence."
   ],
   "id": "5ce299e8-5e09-447a-92d0-f21dd2e515de"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s understand how this synonym replacement works.\n",
    "\n",
    "Suppose we have a sentence “is there an alarm for ten am” Step 1 is to remove the stopwords from the sentence, so now the sentence would be “there alarm ten am”.\n",
    "\n",
    "The length of the sentence now is 4 which is l.\n",
    "\n",
    "Let’s take alpha as 0.6, So, now when we perform calculation we get n = 0.75\\*4, which is equal to 3, So now we will pick three random words and replace then with their synonyms."
   ],
   "id": "9077da8d-af96-48bd-ab5f-c0b285e5261e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when calculating n, there is high probability that the value will be a decimal value and since n can be only an integer, the author never specified that which value of n we are supposed to pick. ie (ceil or floor). Intent classification task has less number of words as input and even if there is difference of one word in the the augmented text due to this ceil, floor confusion, then it may lead to different results.\n",
    "\n",
    "For the data preprocessing we have two notebooks which will focus on both the scenarios taking a ceil value for n and taking a floor value for n.\n",
    "\n",
    "-   [Notebook(DataPreProcess_floor(n))](/3_data_preprocessing_1.ipynb)\n",
    "\n",
    "-   [Notebook(DataPreProcess_ceil(n))](/3_data_preprocessing_2.ipynb)"
   ],
   "id": "f47875ef-5cce-4f9f-8c46-93c69712800b"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
