{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we are at the Stage where we are ready with the data and in the stage where we have to train the model but before that we need to make some modification to the Bert Large model as mentioned in the training and evaluation section in the paper. Bert large uncased model can be finetuned for specific tasks and in our case we are trying to implement a classification model with 64 classes.\n",
    "\n",
    "The code in the notebook will be performing the following steps:\n",
    "\n",
    "-   Load the dataset\n",
    "\n",
    "-   Load BERT Large uncased model from Huggingfaceâ€™s transformer library\n",
    "\n",
    "-   Modify the architecture of the model\n",
    "\n",
    "-   set up the metrics for evaluating the model\n",
    "\n",
    "-   Train the model with three different datasets\n",
    "\n",
    "This notebook uses hyper parameter tuning for all three models and the results produced may have different hyperparameter for all three models."
   ],
   "id": "a9f52fe8-b823-4994-8a30-8278f1a42bc8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import pickle\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoConfig,\n",
    "    BertModel,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AdamW\n",
    "import torch.optim as optim "
   ],
   "id": "c8e1a26f-2882-4bc8-a4a8-971e39accf1e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ],
   "id": "5e0793a4-83d3-4904-8586-44baa6ea05d1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_dataset_tokenized.pkl', 'rb') as file:\n",
    "    train_dataset = pickle.load(file)\n",
    "\n",
    "with open('val_data_tokenized.pkl', 'rb') as file:\n",
    "    val_dataset = pickle.load(file)\n",
    "\n",
    "with open('test_data_tokenized.pkl', 'rb') as file:\n",
    "    test_dataset = pickle.load(file)\n",
    "\n",
    "with open('train_dataset_full_tokenized.pkl', 'rb') as file:\n",
    "    train_dataset_full = [pickle.load(file)]\n",
    "\n",
    "with open('augmented_train_dataset_tokenized.pkl', 'rb') as file:\n",
    "    train_dataset_augmented = pickle.load(file)\n",
    "\n",
    "with open('function_pickle.pkl', 'rb') as f:\n",
    "    create_training_arguments_and_optimizer = pickle.load(f)"
   ],
   "id": "9f579943-120e-4707-bd1e-f00461291640"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the training arguments"
   ],
   "id": "a3a648dd-1866-4c17-a9e9-bfcfb40f5290"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [5e-5, 4e-5, 3e-5, 2e-5]\n",
    "\n",
    "pre_trained_BERTmodel='bert-large-uncased'\n",
    "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
   ],
   "id": "2f17fbc3-3f3d-49d1-8957-ee692b95b9fe"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modifying Bert for our classification Task"
   ],
   "id": "293b2d2d-7cb3-4802-bbcd-c5c28488170e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelWithCustomLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModelWithCustomLossFunction, self).__init__()\n",
    "        self.num_labels = 64\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pre_trained_BERTmodel, num_labels=self.num_labels\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(1024, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "id": "5d914d63-48e6-4fed-ad5c-529af1a13e51"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create train_model function"
   ],
   "id": "0621ac4a-e227-43f3-9543-8e94fd26c67d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_data, args, val_dataset, test_dataset):\n",
    "    BERT_model = BertModelWithCustomLossFunction()\n",
    "    optimizer = AdamW(BERT_model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    trainer = Trainer(\n",
    "        model=BERT_model,\n",
    "        args=args,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=BERT_tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "        optimizers=(optimizer,),\n",
    "    )\n",
    "    trainer.train()\n",
    "    evaluation_metrics = trainer.predict(test_dataset)\n",
    "    accuracy = evaluation_metrics.metrics['test_accuracy']\n",
    "    torch.cuda.empty_cache()\n",
    "    return accuracy"
   ],
   "id": "d19364d7-381c-4917-8285-ab7f9f23023f"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up metrics for accuracy, precision, recall and f1"
   ],
   "id": "81622f26-4fb8-4f6c-9cc8-def2dd237e0a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ],
   "id": "fac34f96-fee3-4d78-95d1-9094018548d1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ],
   "id": "4f1bedae-c9a4-493f-b8de-f1431f5c7af7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "26dca765-bba9-45e2-af87-61cf4bb33a35"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Full dataset Model"
   ],
   "id": "16bd8c0b-75db-4762-b856-668a12dc2674"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_data in train_dataset_full:\n",
    "    best_accuracy = 0\n",
    "    best_lr = learning_rates[0]\n",
    "    for lr in learning_rates:\n",
    "        args = create_training_arguments(lr)\n",
    "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
    "        if accuracy>best_accuracy:\n",
    "          best_lr = lr\n",
    "          best_accuracy = max(accuracy, best_accuracy)\n",
    "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")"
   ],
   "id": "c7ae0c93-4cfd-4938-b340-5bb734d449cd"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training full few shot dataset model"
   ],
   "id": "a00cd1fa-bbd6-46c2-871b-59896622705b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_data in train_dataset:\n",
    "    best_accuracy = 0\n",
    "    best_lr = learning_rates[0]\n",
    "    for lr in learning_rates:\n",
    "        args, optimizer = create_training_arguments(lr)\n",
    "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
    "        if accuracy>best_accuracy:\n",
    "          best_lr = lr\n",
    "          best_accuracy = max(accuracy, best_accuracy)\n",
    "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")"
   ],
   "id": "12efe4e6-a300-4a99-9d34-6b66688bb5c4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Model on Full few shot + Augmented dataset"
   ],
   "id": "f2ef0a6c-aba4-41f9-9609-7316d6d558b2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_data in train_dataset_augmented:\n",
    "    best_accuracy = 0\n",
    "    best_lr = learning_rates[0]\n",
    "    for lr in learning_rates:\n",
    "        args = create_training_arguments(lr)\n",
    "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
    "        if accuracy>best_accuracy:\n",
    "          best_lr = lr\n",
    "          best_accuracy = max(accuracy, best_accuracy)\n",
    "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")"
   ],
   "id": "fa460e40-e58d-465d-9d56-ae9f10be0e40"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
