{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBcS5z+niTN3+BDK1RWPIx"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "W5f2MBrvTVh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI3fPaFpTJFB"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import pickle\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    BertModel,\n",
        ")\n",
        "\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data\n"
      ],
      "metadata": {
        "id": "izrPt_-KWS11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset = pickle.load(file)\n",
        "\n",
        "with open('val_data_tokenized.pkl', 'rb') as file:\n",
        "    val_dataset = pickle.load(file)\n",
        "\n",
        "with open('test_data_tokenized.pkl', 'rb') as file:\n",
        "    test_dataset = pickle.load(file)"
      ],
      "metadata": {
        "id": "hTb-OcZxWXQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the training arguments"
      ],
      "metadata": {
        "id": "VuE3gDswXJ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = TrainingArguments(\n",
        "        output_dir=\"./output\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=3e-5,\n",
        "        per_device_train_batch_size=8 ,\n",
        "        per_device_eval_batch_size=8 ,\n",
        "        num_train_epochs=20,\n",
        "        warmup_ratio= 0.1,\n",
        "        weight_decay= 0.001,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_total_limit=1,\n",
        "            )\n",
        "\n",
        "pre_trained_BERTmodel='bert-large-uncased'"
      ],
      "metadata": {
        "id": "xhvhFJ5jTvX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying Bert for our classification Task"
      ],
      "metadata": {
        "id": "CMpQb_ihXcg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModelWithCustomLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertModelWithCustomLossFunction, self).__init__()\n",
        "        self.num_labels = len(df_train[\"intent\"].unique())\n",
        "        self.bert = BertModel.from_pretrained(\n",
        "            pre_trained_BERTmodel, num_labels=self.num_labels\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(1024, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        output = self.dropout(outputs.pooler_output)\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "Tcq_VLJkTzPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up metrics for accuracy, precision, recall and f1"
      ],
      "metadata": {
        "id": "EUDt--_4MlLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "lLl0M8rkUJcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Fegvkm8_N7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_accuracy = 0\n",
        "for train_data in train_dataset:\n",
        "  BERT_model = BertModelWithCustomLossFunction()\n",
        "  trainer = Trainer(\n",
        "        model = BERT_model,\n",
        "        args = args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=BERT_tokenizer,\n",
        "        compute_metrics=compute_metrics,)\n",
        "  trainer.train()\n",
        "  evaluation_metrics = trainer.predict(test_dataset)\n",
        "  accuracy = evaluation_metrics.metrics['test_accuracy']\n",
        "  best_accuracy = max(accuracy, best_accuracy)\n",
        "  print(f\"Best Test Accuracy for this training dataset: {accuracy}\")\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "W8dv_HOcVKE0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}