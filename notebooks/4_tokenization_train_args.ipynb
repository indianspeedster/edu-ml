{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUNd1tIoFDZJhuZnXA++Sj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XOXhkgvgK99B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the relevant libraries\n"
      ],
      "metadata": {
        "id": "NVi9hWEpLOGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QryWPN7K0Xe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up tokenizer"
      ],
      "metadata": {
        "id": "g_UOgiQ0Lc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "XlBBAL-1LmAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "qoP7xgtULwfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'rb') as file:\n",
        "    training_datasets = pickle.load(file)\n",
        "with open('val_data.pkl', 'rb') as file:\n",
        "    val_data = pickle.load(file)\n",
        "with open('test_data.pkl', 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "with open('augmented_datasets.pkl', 'rb') as file:\n",
        "    augmented_datasets = pickle.load(file)\n",
        "with open('train_data_full.pkl', 'rb') as file:\n",
        "    train_data_full = pickle.load(file)"
      ],
      "metadata": {
        "id": "8lJxdsNpLy6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the data\n"
      ],
      "metadata": {
        "id": "PXMQUoASMZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example):\n",
        "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
      ],
      "metadata": {
        "id": "si6mvOmWMcx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing non augmented training data"
      ],
      "metadata": {
        "id": "_R6qW224MoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "for train_data_ in training_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "-t_9VSjpMsbr",
        "outputId": "8c00d5a8-06f3-48ac-88aa-3da1d16eab94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3543.66 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3657.62 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3634.55 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing augmented training data"
      ],
      "metadata": {
        "id": "4MdQV9sNMwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_dataset=[]\n",
        "for train_data_ in augmented_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "yyedMqXUM4lD",
        "outputId": "fd6f2aa9-7927-425b-86e4-712311b31bd3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3565.57 examples/s]\n",
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3558.46 examples/s]\n",
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3578.60 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing validation data"
      ],
      "metadata": {
        "id": "44sOlCHnNCAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "val_data = val_data.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jaAOQcNENGY1",
        "outputId": "7333f2a9-295a-49c9-8f83-80c801a75f8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 993/993 [00:00<00:00, 3547.52 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing test data"
      ],
      "metadata": {
        "id": "Hdp_mlO1NSX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = datasets.Dataset.from_pandas(test_data)\n",
        "test_dataset = testdataset.map(tokenize_data)"
      ],
      "metadata": {
        "id": "4IgiUYjlNXLL",
        "outputId": "b691a134-6a9a-4267-d3cd-1ddaa3f7855a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1075/1075 [00:00<00:00, 3596.62 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize full train dataset"
      ],
      "metadata": {
        "id": "O0C8O32lDxvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_full = datasets.Dataset.from_pandas(train_data_full)\n",
        "train_data_full = train_data_full.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jhY6ykAoDxWj",
        "outputId": "d25fda4a-fa8a-4019-fc16-aa81ef52dc02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 8934/8934 [00:02<00:00, 3564.70 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Training arguments\n",
        "#### Training Arguments Explanation\n",
        "\n",
        "1. **`output_dir`**: Specifies the directory where model checkpoints and training logs will be saved.\n",
        "\n",
        "2. **`evaluation_strategy`**: Defines the strategy for evaluating the model during training. Here, it's set to \"epoch,\" meaning evaluation occurs after each epoch.\n",
        "\n",
        "3. **`save_strategy`**: Specifies when to save model checkpoints. In this case, it's set to \"epoch,\" indicating checkpoints are saved after each epoch.\n",
        "\n",
        "4. **`learning_rate`**: Determines the step size at which the optimizer adjusts model weights during training.\n",
        "\n",
        "5. **`per_device_train_batch_size`**: Specifies the batch size for training data per GPU, impacting memory usage and computational efficiency.\n",
        "\n",
        "6. **`per_device_eval_batch_size`**: Sets the batch size for evaluation data per GPU, affecting memory and computation during evaluation.\n",
        "\n",
        "7. **`num_train_epochs`**: Indicates the total number of training epochs, which are complete passes through the training dataset.\n",
        "\n",
        "8. **`warmup_ratio`**: Determines the ratio of warmup steps to the total number of training steps, helping the optimizer to smoothly adapt in the initial stages of training.\n",
        "\n",
        "9. **`weight_decay`**: Introduces L2 regularization to the optimizer, helping to prevent overfitting by penalizing large model weights.\n",
        "\n",
        "10. **`load_best_model_at_end`**: Specifies whether to load the best model based on the chosen evaluation metric at the end of training.\n",
        "\n",
        "11. **`metric_for_best_model`**: Specifies the evaluation metric used to determine the best model, which is set to \"accuracy\" in this case.\n",
        "\n",
        "12. **`save_total_limit`**: Sets the maximum number of model checkpoints to keep, preventing excessive storage usage.\n",
        "\n",
        "13. **`logging_dir`**: Designates the directory where training logs, such as training progress and performance metrics, will be stored.\n",
        "\n",
        "14. **`optimizers`**: Specifies the optimizers used for model parameter updates during training, allowing for gradient-based optimization algorithms like AdamW, SGD and many more.\n"
      ],
      "metadata": {
        "id": "PTKIngh_jld3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_training_arguments_and_optimizer(lr):\n",
        "    args = TrainingArguments(\n",
        "        output_dir=\"./output\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        num_train_epochs=3,\n",
        "        warmup_ratio=0.1,\n",
        "        weight_decay=0.001,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_total_limit=1,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    optimizer = AdamW(args.model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "\n",
        "    return args, optimizer"
      ],
      "metadata": {
        "id": "FYF8FGFIlbt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store tokenized data and Training Arguments"
      ],
      "metadata": {
        "id": "oZPsyQu7NlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "with open('val_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(test_dataset, file)\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_train_dataset, file)\n",
        "with open('train_dataset_full_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_data_full, file)\n",
        "with open('function_train_args.pkl', 'wb') as f:\n",
        "    pickle.dump(create_training_arguments_and_optimizer, f)"
      ],
      "metadata": {
        "id": "rYLLCvlbNn0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n",
        "\n",
        "This notebook will generate 5 files as mentioned below :\n",
        "\n",
        "- train_dataset_tokenized.pkl\n",
        "\n",
        "- val_data_tokenized.pkl\n",
        "\n",
        "- test_data_tokenized.pkl\n",
        "\n",
        "- augmented_train_dataset_tokenized.pkl\n",
        "\n",
        "- train_dataset_full_tokenized.pkl"
      ],
      "metadata": {
        "id": "z-JkrGG-Oe4q"
      }
    }
  ]
}