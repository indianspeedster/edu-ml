{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzQdNCt4mVHG1BwcVAumHZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "W5f2MBrvTVh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xI3fPaFpTJFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3ad751-7417-48fc-b77e-d31a2bffd74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-07-26 03:13:09.437773: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import pickle\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    BertModel,\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data\n"
      ],
      "metadata": {
        "id": "izrPt_-KWS11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset = pickle.load(file)\n",
        "\n",
        "with open('val_data_tokenized.pkl', 'rb') as file:\n",
        "    val_dataset = pickle.load(file)\n",
        "\n",
        "with open('test_data_tokenized.pkl', 'rb') as file:\n",
        "    test_dataset = pickle.load(file)\n",
        "\n",
        "with open('train_dataset_full_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset_full = [pickle.load(file)]\n",
        "\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset_augmented = pickle.load(file)"
      ],
      "metadata": {
        "id": "hTb-OcZxWXQM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the training arguments"
      ],
      "metadata": {
        "id": "VuE3gDswXJ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [5e-5, 4e-5, 3e-5, 2e-5]\n",
        "\n",
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "xhvhFJ5jTvX2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying Bert for our classification Task"
      ],
      "metadata": {
        "id": "CMpQb_ihXcg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModelWithCustomLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertModelWithCustomLossFunction, self).__init__()\n",
        "        self.num_labels = 64\n",
        "        self.bert = BertModel.from_pretrained(\n",
        "            pre_trained_BERTmodel, num_labels=self.num_labels\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(1024, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        output = self.dropout(outputs.pooler_output)\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "Tcq_VLJkTzPg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up metrics for accuracy, precision, recall and f1"
      ],
      "metadata": {
        "id": "EUDt--_4MlLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "lLl0M8rkUJcF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Fegvkm8_N7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "99kiv5ZgLIgx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, train in enumerate([train_dataset_full, train_dataset, train_dataset_augmented]):\n",
        "  best_lr = learning_rates[0]\n",
        "  for lr in learning_rates:\n",
        "    if i != 0:\n",
        "      learning_rate = best_lr\n",
        "    else:\n",
        "      learning_rate = lr\n",
        "    best_accuracy = 0\n",
        "    for train_data in train:\n",
        "      BERT_model = BertModelWithCustomLossFunction()\n",
        "      trainer = Trainer(\n",
        "            model = BERT_model,\n",
        "            args = TrainingArguments(\n",
        "          output_dir=\"./output\",\n",
        "          evaluation_strategy=\"epoch\",\n",
        "          save_strategy=\"epoch\",\n",
        "          learning_rate=learning_rate,\n",
        "          per_device_train_batch_size=8 ,\n",
        "          per_device_eval_batch_size=8 ,\n",
        "          num_train_epochs=3,\n",
        "          warmup_ratio= 0.1,\n",
        "          weight_decay= 0.001,\n",
        "          load_best_model_at_end=True,\n",
        "          metric_for_best_model=\"accuracy\",\n",
        "          save_total_limit=1,\n",
        "              ),\n",
        "            train_dataset=train_data,\n",
        "            eval_dataset=val_dataset,\n",
        "            tokenizer=BERT_tokenizer,\n",
        "            compute_metrics=compute_metrics,)\n",
        "      trainer.train()\n",
        "      evaluation_metrics = trainer.predict(test_dataset)\n",
        "      accuracy = evaluation_metrics.metrics['test_accuracy']\n",
        "      if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        if i == 0:\n",
        "          best_lr = learning_rate\n",
        "      best_accuracy = max(accuracy, best_accuracy)\n",
        "      torch.cuda.empty_cache()\n",
        "    print(f\"Best Test Accuracy for this training dataset: {accuracy}\")"
      ],
      "metadata": {
        "id": "W8dv_HOcVKE0",
        "collapsed": true,
        "outputId": "1396d541-1c29-42bd-849e-d70fd63f05bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='840' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [840/840 19:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.584568</td>\n",
              "      <td>0.875126</td>\n",
              "      <td>0.885359</td>\n",
              "      <td>0.875126</td>\n",
              "      <td>0.869749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.415400</td>\n",
              "      <td>0.356534</td>\n",
              "      <td>0.915408</td>\n",
              "      <td>0.920410</td>\n",
              "      <td>0.915408</td>\n",
              "      <td>0.914232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.415400</td>\n",
              "      <td>0.300870</td>\n",
              "      <td>0.934542</td>\n",
              "      <td>0.938047</td>\n",
              "      <td>0.934542</td>\n",
              "      <td>0.933770</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Test Accuracy for this training dataset: 0.9144186046511628\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='584' max='840' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [584/840 13:27 < 05:55, 0.72 it/s, Epoch 2.08/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.654746</td>\n",
              "      <td>0.868077</td>\n",
              "      <td>0.869433</td>\n",
              "      <td>0.868077</td>\n",
              "      <td>0.857108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.588600</td>\n",
              "      <td>0.376920</td>\n",
              "      <td>0.903323</td>\n",
              "      <td>0.902579</td>\n",
              "      <td>0.903323</td>\n",
              "      <td>0.899305</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}