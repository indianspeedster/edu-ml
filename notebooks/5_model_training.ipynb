{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfsbina7XUtldYFy6yZda3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "W5f2MBrvTVh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI3fPaFpTJFB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd3ad751-7417-48fc-b77e-d31a2bffd74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-07-26 03:13:09.437773: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "import pickle\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoConfig,\n",
        "    BertModel,\n",
        ")\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "from transformers import AdamW"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data\n"
      ],
      "metadata": {
        "id": "izrPt_-KWS11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset = pickle.load(file)\n",
        "\n",
        "with open('val_data_tokenized.pkl', 'rb') as file:\n",
        "    val_dataset = pickle.load(file)\n",
        "\n",
        "with open('test_data_tokenized.pkl', 'rb') as file:\n",
        "    test_dataset = pickle.load(file)\n",
        "\n",
        "with open('train_dataset_full_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset_full = [pickle.load(file)]\n",
        "\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'rb') as file:\n",
        "    train_dataset_augmented = pickle.load(file)\n",
        "\n",
        "with open('function_pickle.pkl', 'rb') as f:\n",
        "    create_training_arguments_and_optimizer = pickle.load(f)"
      ],
      "metadata": {
        "id": "hTb-OcZxWXQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the training arguments"
      ],
      "metadata": {
        "id": "VuE3gDswXJ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [5e-5, 4e-5, 3e-5, 2e-5]\n",
        "\n",
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "xhvhFJ5jTvX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying Bert for our classification Task"
      ],
      "metadata": {
        "id": "CMpQb_ihXcg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertModelWithCustomLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertModelWithCustomLossFunction, self).__init__()\n",
        "        self.num_labels = 64\n",
        "        self.bert = BertModel.from_pretrained(\n",
        "            pre_trained_BERTmodel, num_labels=self.num_labels\n",
        "        )\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(1024, self.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        output = self.dropout(outputs.pooler_output)\n",
        "        logits = self.classifier(output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ],
      "metadata": {
        "id": "Tcq_VLJkTzPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train_model function"
      ],
      "metadata": {
        "id": "Zt575E8Hu_39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_data, args, val_dataset, test_dataset):\n",
        "    BERT_model = BertModelWithCustomLossFunction()\n",
        "    optimizer = AdamW(BERT_model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
        "    trainer = Trainer(\n",
        "        model=BERT_model,\n",
        "        args=args,\n",
        "        train_dataset=train_data,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=BERT_tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        optimizers=(optimizer,),\n",
        "    )\n",
        "    trainer.train()\n",
        "    evaluation_metrics = trainer.predict(test_dataset)\n",
        "    accuracy = evaluation_metrics.metrics['test_accuracy']\n",
        "    torch.cuda.empty_cache()\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "ouZNiPHVu_Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up metrics for accuracy, precision, recall and f1"
      ],
      "metadata": {
        "id": "EUDt--_4MlLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "lLl0M8rkUJcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "Fegvkm8_N7Ya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "99kiv5ZgLIgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Full dataset Model"
      ],
      "metadata": {
        "id": "aRAGQIDQ0S5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_data in train_dataset_full:\n",
        "    best_accuracy = 0\n",
        "    best_lr = learning_rates[0]\n",
        "    for lr in learning_rates:\n",
        "        args = create_training_arguments(lr)\n",
        "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
        "        if accuracy>best_accuracy:\n",
        "          best_lr = lr\n",
        "          best_accuracy = max(accuracy, best_accuracy)\n",
        "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")\n",
        ""
      ],
      "metadata": {
        "id": "wk3BA_d7fz8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training full few shot dataset model"
      ],
      "metadata": {
        "id": "okrykP7t0YR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_data in train_dataset:\n",
        "    best_accuracy = 0\n",
        "    best_lr = learning_rates[0]\n",
        "    for lr in learning_rates:\n",
        "        args, optimizer = create_training_arguments(lr)\n",
        "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
        "        if accuracy>best_accuracy:\n",
        "          best_lr = lr\n",
        "          best_accuracy = max(accuracy, best_accuracy)\n",
        "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")"
      ],
      "metadata": {
        "id": "6TB4oZg3ygBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Model on Full few shot + Augmented dataset"
      ],
      "metadata": {
        "id": "1udbUtcM0cmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_data in train_dataset_augmented:\n",
        "    best_accuracy = 0\n",
        "    best_lr = learning_rates[0]\n",
        "    for lr in learning_rates:\n",
        "        args = create_training_arguments(lr)\n",
        "        accuracy = train_model(train_data, args, val_dataset, test_dataset)\n",
        "        if accuracy>best_accuracy:\n",
        "          best_lr = lr\n",
        "          best_accuracy = max(accuracy, best_accuracy)\n",
        "print(f\"Best Accuracy:{best_accuracy}\\n Best Learning Rate: {best_lr}\")"
      ],
      "metadata": {
        "id": "h9kZ3acryhwT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}