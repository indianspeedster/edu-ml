{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJftysCmKqAsbnjnuKFcoI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XOXhkgvgK99B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the relevant libraries\n"
      ],
      "metadata": {
        "id": "NVi9hWEpLOGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5QryWPN7K0Xe",
        "outputId": "7a00a02c-cfe9-41a8-c25c-24cd40df3beb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/cc/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up tokenizer"
      ],
      "metadata": {
        "id": "g_UOgiQ0Lc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "XlBBAL-1LmAB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "qoP7xgtULwfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'rb') as file:\n",
        "    training_datasets = pickle.load(file)\n",
        "with open('val_data.pkl', 'rb') as file:\n",
        "    val_data = pickle.load(file)\n",
        "with open('test_data.pkl', 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "with open('augmented_datasets.pkl', 'rb') as file:\n",
        "    augmented_datasets = pickle.load(file)\n",
        "with open('train_data_full.pkl', 'rb') as file:\n",
        "    train_data_full = pickle.load(file)"
      ],
      "metadata": {
        "id": "8lJxdsNpLy6-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the data\n"
      ],
      "metadata": {
        "id": "PXMQUoASMZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example):\n",
        "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
      ],
      "metadata": {
        "id": "si6mvOmWMcx8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing non augmented training data"
      ],
      "metadata": {
        "id": "_R6qW224MoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "for train_data_ in training_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "-t_9VSjpMsbr",
        "outputId": "b29fa978-157b-4b5b-8dbd-7d78a45995a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3288.22 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3677.56 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3668.11 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing augmented training data"
      ],
      "metadata": {
        "id": "4MdQV9sNMwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_dataset=[]\n",
        "for train_data_ in augmented_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "yyedMqXUM4lD",
        "outputId": "6f869001-d76f-49c0-a555-51c7febe6272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3663.95 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3680.07 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3668.03 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing validation data"
      ],
      "metadata": {
        "id": "44sOlCHnNCAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "val_data = val_data.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jaAOQcNENGY1",
        "outputId": "21b8fca3-8d5d-4af4-9bba-8657cd1a68d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 993/993 [00:00<00:00, 3568.06 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing test data"
      ],
      "metadata": {
        "id": "Hdp_mlO1NSX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = datasets.Dataset.from_pandas(test_data)\n",
        "test_dataset = testdataset.map(tokenize_data)"
      ],
      "metadata": {
        "id": "4IgiUYjlNXLL",
        "outputId": "e952370d-2214-4010-9310-992beb911467",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1075/1075 [00:00<00:00, 3548.38 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize full train dataset"
      ],
      "metadata": {
        "id": "O0C8O32lDxvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_full = datasets.Dataset.from_pandas(train_data_full)\n",
        "train_data_full = train_data_full.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jhY6ykAoDxWj",
        "outputId": "fb8f01a4-e150-4b79-b9d9-2ba4d6f23028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 9927/9927 [00:02<00:00, 3670.86 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store tokenized data"
      ],
      "metadata": {
        "id": "oZPsyQu7NlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "with open('val_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(test_dataset, file)\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_train_dataset, file)"
      ],
      "metadata": {
        "id": "rYLLCvlbNn0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n",
        "\n",
        "This notebook will generate 4 files as mentioned below :\n",
        "\n",
        "- train_dataset_tokenized.pkl\n",
        "\n",
        "- val_data_tokenized.pkl\n",
        "\n",
        "- test_data_tokenized.pkl\n",
        "\n",
        "- augmented_train_dataset_tokenized.pkl"
      ],
      "metadata": {
        "id": "z-JkrGG-Oe4q"
      }
    }
  ]
}