{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNRzY+tfXbGCl0liHwUvKm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XOXhkgvgK99B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the relevant libraries\n"
      ],
      "metadata": {
        "id": "NVi9hWEpLOGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QryWPN7K0Xe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import pikle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up tokenizer"
      ],
      "metadata": {
        "id": "g_UOgiQ0Lc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "XlBBAL-1LmAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "qoP7xgtULwfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'rb') as file:\n",
        "    training_datasets = pickle.load(file)\n",
        "with open('val_data.pkl', 'rb') as file:\n",
        "    val_data = pickle.load(file)\n",
        "with open('test_data.pkl', 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "with open('augmented_datasets.pkl', 'rb') as file:\n",
        "    augmented_datasets = pickle.load(file)"
      ],
      "metadata": {
        "id": "8lJxdsNpLy6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the data\n"
      ],
      "metadata": {
        "id": "PXMQUoASMZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example):\n",
        "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
      ],
      "metadata": {
        "id": "si6mvOmWMcx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing non augmented training data"
      ],
      "metadata": {
        "id": "_R6qW224MoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "for train_data_ in training_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "-t_9VSjpMsbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing augmented training data"
      ],
      "metadata": {
        "id": "4MdQV9sNMwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_dataset=[]\n",
        "for train_data_ in augmented_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "yyedMqXUM4lD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing validation data"
      ],
      "metadata": {
        "id": "44sOlCHnNCAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "val_data = testdataset.map(val_data)"
      ],
      "metadata": {
        "id": "jaAOQcNENGY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing test data"
      ],
      "metadata": {
        "id": "Hdp_mlO1NSX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = datasets.Dataset.from_pandas(test_data)\n",
        "test_dataset = testdataset.map(tokenize_data)"
      ],
      "metadata": {
        "id": "4IgiUYjlNXLL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store tokenized data"
      ],
      "metadata": {
        "id": "oZPsyQu7NlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "with open('val_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(test_data, file)\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_train_dataset, file)"
      ],
      "metadata": {
        "id": "rYLLCvlbNn0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n",
        "\n",
        "This notebook will generate 4 files as mentioned below :\n",
        "\n",
        "- train_dataset_tokenized.pkl\n",
        "\n",
        "- val_data_tokenized.pkl\n",
        "\n",
        "- test_data_tokenized.pkl\n",
        "\n",
        "- augmented_train_dataset_tokenized.pkl"
      ],
      "metadata": {
        "id": "z-JkrGG-Oe4q"
      }
    }
  ]
}
