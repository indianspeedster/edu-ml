{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO6lhZp4yF0GY91M82c2Wt7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XOXhkgvgK99B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the relevant libraries\n"
      ],
      "metadata": {
        "id": "NVi9hWEpLOGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5QryWPN7K0Xe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up tokenizer"
      ],
      "metadata": {
        "id": "g_UOgiQ0Lc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "XlBBAL-1LmAB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "qoP7xgtULwfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'rb') as file:\n",
        "    training_datasets = pickle.load(file)\n",
        "with open('val_data.pkl', 'rb') as file:\n",
        "    val_data = pickle.load(file)\n",
        "with open('test_data.pkl', 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "with open('augmented_datasets.pkl', 'rb') as file:\n",
        "    augmented_datasets = pickle.load(file)\n",
        "with open('train_data_full.pkl', 'rb') as file:\n",
        "    train_data_full = pickle.load(file)"
      ],
      "metadata": {
        "id": "8lJxdsNpLy6-"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the data\n"
      ],
      "metadata": {
        "id": "PXMQUoASMZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example):\n",
        "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
      ],
      "metadata": {
        "id": "si6mvOmWMcx8"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing non augmented training data"
      ],
      "metadata": {
        "id": "_R6qW224MoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "for train_data_ in training_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "-t_9VSjpMsbr",
        "outputId": "560e003d-549e-43c2-a40e-350a5a9249fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3488.59 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3632.38 examples/s]\n",
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 640/640 [00:00<00:00, 3693.89 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing augmented training data"
      ],
      "metadata": {
        "id": "4MdQV9sNMwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_dataset=[]\n",
        "for train_data_ in augmented_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "yyedMqXUM4lD",
        "outputId": "e489a015-88de-4e3d-a930-9dc868a818ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3566.96 examples/s]\n",
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3610.75 examples/s]\n",
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1280/1280 [00:00<00:00, 3578.31 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing validation data"
      ],
      "metadata": {
        "id": "44sOlCHnNCAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "val_data = val_data.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jaAOQcNENGY1",
        "outputId": "47449abf-c315-427d-c6ca-7cc417285195",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████████████████████████████████████████████████████████████████████| 993/993 [00:00<00:00, 3587.75 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing test data"
      ],
      "metadata": {
        "id": "Hdp_mlO1NSX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = datasets.Dataset.from_pandas(test_data)\n",
        "test_dataset = testdataset.map(tokenize_data)"
      ],
      "metadata": {
        "id": "4IgiUYjlNXLL",
        "outputId": "6de24f29-3fa5-4b8e-9168-a40296ce3d4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 1075/1075 [00:00<00:00, 3573.40 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenize full train dataset"
      ],
      "metadata": {
        "id": "O0C8O32lDxvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_full = datasets.Dataset.from_pandas(train_data_full)\n",
        "train_data_full = train_data_full.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jhY6ykAoDxWj",
        "outputId": "a29b2115-9468-41dd-b889-4be69e691844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|████████████████████████████████████████████████████████████████████████| 9927/9927 [00:02<00:00, 3578.87 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store tokenized data"
      ],
      "metadata": {
        "id": "oZPsyQu7NlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "with open('val_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(test_dataset, file)\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_train_dataset, file)\n",
        "with open('train_dataset_full_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_data_full, file)"
      ],
      "metadata": {
        "id": "rYLLCvlbNn0b"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n",
        "\n",
        "This notebook will generate 5 files as mentioned below :\n",
        "\n",
        "- train_dataset_tokenized.pkl\n",
        "\n",
        "- val_data_tokenized.pkl\n",
        "\n",
        "- test_data_tokenized.pkl\n",
        "\n",
        "- augmented_train_dataset_tokenized.pkl\n",
        "\n",
        "- train_dataset_full_tokenized.pkl"
      ],
      "metadata": {
        "id": "z-JkrGG-Oe4q"
      }
    }
  ]
}