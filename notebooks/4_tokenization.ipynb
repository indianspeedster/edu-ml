{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWvSV0M+9VAxQACLdLczLT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "XOXhkgvgK99B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the relevant libraries\n"
      ],
      "metadata": {
        "id": "NVi9hWEpLOGu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "5QryWPN7K0Xe"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import datasets\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up tokenizer"
      ],
      "metadata": {
        "id": "g_UOgiQ0Lc0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_BERTmodel='bert-large-uncased'\n",
        "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)"
      ],
      "metadata": {
        "id": "XlBBAL-1LmAB"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading data"
      ],
      "metadata": {
        "id": "qoP7xgtULwfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'rb') as file:\n",
        "    training_datasets = pickle.load(file)\n",
        "with open('val_data.pkl', 'rb') as file:\n",
        "    val_data = pickle.load(file)\n",
        "with open('test_data.pkl', 'rb') as file:\n",
        "    test_data = pickle.load(file)\n",
        "with open('augmented_datasets.pkl', 'rb') as file:\n",
        "    augmented_datasets = pickle.load(file)"
      ],
      "metadata": {
        "id": "8lJxdsNpLy6-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to tokenize the data\n"
      ],
      "metadata": {
        "id": "PXMQUoASMZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_data(example):\n",
        "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
        "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
      ],
      "metadata": {
        "id": "si6mvOmWMcx8"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing non augmented training data"
      ],
      "metadata": {
        "id": "_R6qW224MoUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=[]\n",
        "for train_data_ in training_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "-t_9VSjpMsbr",
        "outputId": "76f19591-4ea7-4905-f35c-ee228a3af4b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3406.86 examples/s]\n",
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3479.71 examples/s]\n",
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3542.67 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing augmented training data"
      ],
      "metadata": {
        "id": "4MdQV9sNMwAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_train_dataset=[]\n",
        "for train_data_ in augmented_datasets:\n",
        "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
        "  augmented_train_dataset.append(traindataset.map(tokenize_data))"
      ],
      "metadata": {
        "id": "yyedMqXUM4lD",
        "outputId": "9940ce45-3f86-47b7-a9ab-58bd9e23f2f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3465.59 examples/s]\n",
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3522.57 examples/s]\n",
            "Map: 100%|██████████| 640/640 [00:00<00:00, 3497.46 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing validation data"
      ],
      "metadata": {
        "id": "44sOlCHnNCAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_data = datasets.Dataset.from_pandas(val_data)\n",
        "val_data = val_data.map(tokenize_data)"
      ],
      "metadata": {
        "id": "jaAOQcNENGY1",
        "outputId": "28020e11-1e1c-4b24-9fc9-0fde02a4e84a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 993/993 [00:00<00:00, 3460.64 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Tokenizing test data"
      ],
      "metadata": {
        "id": "Hdp_mlO1NSX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testdataset = datasets.Dataset.from_pandas(test_data)\n",
        "test_dataset = testdataset.map(tokenize_data)"
      ],
      "metadata": {
        "id": "4IgiUYjlNXLL",
        "outputId": "1e527a16-6dcd-488b-9528-54856e198a0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map: 100%|██████████| 1075/1075 [00:00<00:00, 3522.23 examples/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store tokenized data"
      ],
      "metadata": {
        "id": "oZPsyQu7NlB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(train_dataset, file)\n",
        "with open('val_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(test_dataset, file)\n",
        "with open('augmented_train_dataset_tokenized.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_train_dataset, file)"
      ],
      "metadata": {
        "id": "rYLLCvlbNn0b"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output\n",
        "\n",
        "\n",
        "This notebook will generate 4 files as mentioned below :\n",
        "\n",
        "- train_dataset_tokenized.pkl\n",
        "\n",
        "- val_data_tokenized.pkl\n",
        "\n",
        "- test_data_tokenized.pkl\n",
        "\n",
        "- augmented_train_dataset_tokenized.pkl"
      ],
      "metadata": {
        "id": "z-JkrGG-Oe4q"
      }
    }
  ]
}