{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOu+sitKjvGEbyHzhZESfF5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Importing relevent libraries\n"
      ],
      "metadata": {
        "id": "7AT4LoEExHtj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "seed=123"
      ],
      "metadata": {
        "id": "-ptStdYRw9tt",
        "outputId": "381233f3-418a-411b-eac5-75c06d495bb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /home/cc/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /home/cc/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the train and test data"
      ],
      "metadata": {
        "id": "0Ku4YVZnyh7J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvm-c9EDlYiT"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode the labels"
      ],
      "metadata": {
        "id": "VeZ1D_pmgFkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le=LabelEncoder()\n",
        "train_data['intent']=le.fit_transform(train_data['intent'])\n",
        "test_data['intent']=le.transform(test_data['intent'])\n",
        "train_data = train_data.drop(\"Unnamed: 0\", axis=1)"
      ],
      "metadata": {
        "id": "gCQnd4aHAEKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split the training data to train and validation"
      ],
      "metadata": {
        "id": "SwX93Ogaae-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train,val_data=train_test_split(train_data,test_size=0.10 ,random_state=seed, shuffle=True)"
      ],
      "metadata": {
        "id": "oPDAyK3vaYD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get random 30 samples from training data"
      ],
      "metadata": {
        "id": "gK3DSlA2aoGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the unique intent\n",
        "unique_labels = df_train['intent'].unique()\n",
        "# Creating an empty dataframe to store all the values\n",
        "sampled_df = pd.DataFrame()\n",
        "#Iterating through each label and take random 30 samples from it\n",
        "for label in unique_labels:\n",
        "    label_df = df_train[df_train['intent'] == label]\n",
        "    samples = label_df.sample(n=30, random_state=seed)\n",
        "    sampled_df = sampled_df.append(samples)\n",
        "sampled_df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "mXXTSrClaYAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### create 3 unique 10-shot dataset from previous sampled data"
      ],
      "metadata": {
        "id": "9ERzgD6vcJhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = sampled_df\n",
        "# Create a column sample and mark it all as False and when you pick a sample mark them as True. This will make sure that you are not repeating the same sample again.\n",
        "df['sampled'] = False\n",
        "\n",
        "#creating a list to store the 10 shot dataset\n",
        "training_datasets = []\n",
        "\n",
        "for i in range(3):\n",
        "    dataset = pd.DataFrame()\n",
        "    for label in df['intent'].unique():\n",
        "        label_df = df[(df['intent'] == label) & (df['sampled'] == False)]\n",
        "        if len(label_df) >= 10:\n",
        "            samples = label_df.sample(n=10)\n",
        "            df.loc[samples.index, 'sampled'] = True\n",
        "            dataset = pd.concat([dataset, samples])\n",
        "        else:\n",
        "            samples = label_df\n",
        "            df.loc[samples.index, 'sampled'] = True\n",
        "            dataset = pd.concat([dataset, samples])\n",
        "    dataset = dataset.drop(\"sampled\",axis=1)\n",
        "    dataset = dataset.reset_index(drop=True)\n",
        "    training_datasets.append(dataset)\n",
        "\n",
        "# The output of this cell will create a list training_datasets which contains 3 10-shot dataset"
      ],
      "metadata": {
        "id": "XTnJZMMzaX56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store data"
      ],
      "metadata": {
        "id": "WJRspPxDGZpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('training_datasets.pkl', 'wb') as file:\n",
        "    pickle.dump(training_datasets, file)\n",
        "with open('val_data.pkl', 'wb') as file:\n",
        "    pickle.dump(val_data, file)\n",
        "with open('test_data.pkl', 'wb') as file:\n",
        "    pickle.dump(test_data, file)\n",
        "with open('train_data_full.pkl', 'wb') as file:\n",
        "    pickle.dump(train_data, file)"
      ],
      "metadata": {
        "id": "2e47mGV7GW9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Augmentation\n",
        "\n",
        "Upper and lower bound of n and alpha = 0.5 & 0.75"
      ],
      "metadata": {
        "id": "ZFUGQj2n3oir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#loading stop words\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "9GR6Gi5laXzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Augmentation function\n"
      ],
      "metadata": {
        "id": "NG6VXSxHy0MS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def augmentation(sentence, alpha=0.5 ):\n",
        "  sentence = sentence.split(\" \")\n",
        "  word_index = [i for i in range(len(sentence)) if sentence[i].lower() not in stop_words]\n",
        "  n = int(alpha*len(word_index))\n",
        "  n_random = random.sample(word_index, n)\n",
        "  for num in n_random:\n",
        "    word = sentence[num]\n",
        "    synonyms = []\n",
        "    for synset in wordnet.synsets(word):\n",
        "      for synonym in synset.lemmas():\n",
        "        synonyms.append(synonym.name())\n",
        "    if len(synonyms)>=2:\n",
        "      sentence[num] = synonyms[1]\n",
        "    else:\n",
        "      pass\n",
        "  return \" \".join(sentence)"
      ],
      "metadata": {
        "id": "2Jhg0j3HVmcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Apply data augmentation on each of the three training datasets"
      ],
      "metadata": {
        "id": "bZTMDe3rIOLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_datasets = []\n",
        "for train_data in training_datasets:\n",
        "  augmented_data = train_data.copy()\n",
        "  augmented_data[\"speech_text\"] = augmented_data[\"speech_text\"].apply(augmentation, alpha=0.6)\n",
        "  augmented_data = pd.concat([train_data, augmented_data])\n",
        "  augmented_datasets.append(augmented_data)"
      ],
      "metadata": {
        "id": "K7l_uRktF013"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Store the augmented_data\n"
      ],
      "metadata": {
        "id": "LBzUVScWJiT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('augmented_datasets.pkl', 'wb') as file:\n",
        "    pickle.dump(augmented_datasets, file)"
      ],
      "metadata": {
        "id": "0TGh75dLJsjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output of this Notebook\n",
        "\n",
        "This notebook will generate 4 files as mentioned below :\n",
        "\n",
        "- training_datasets.pkl\n",
        "\n",
        "- val_data.pkl\n",
        "\n",
        "- test_data.pkl\n",
        "\n",
        "- augmented_datasets.pkl"
      ],
      "metadata": {
        "id": "Xo_5S4FXKEyw"
      }
    }
  ]
}