{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install relevent libraries"
   ],
   "id": "8a8877e7-ff13-4845-9631-8606d68c9f06"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install accelerate -U\n",
    "!pip install nltk"
   ],
   "id": "519683bf-8af0-42a5-a188-8647e79824dc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional step\n",
    "\n",
    "Restart the kernel as accelerate requires restart once itâ€™s been installed"
   ],
   "id": "809d7da2-f957-4c14-9d96-22ff4de5fd58"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevent libraries"
   ],
   "id": "c703faba-e9a9-4c0c-ba9c-c5736da01c01"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoConfig,\n",
    "    BertModel,\n",
    ")\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import datasets\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AdamW, AdamWeightDecay, get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from statistics import median\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "seed = 1331"
   ],
   "id": "708d0b9d-5af7-4281-b1fa-c62da5bd23a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "dedc06d5-db87-46c3-9594-1e857ecb6604"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and store the data"
   ],
   "id": "f15da669-6176-4cb6-9980-afb90b599226"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_HEADER = [\"text\", \"category\"]\n",
    "PATTERNS = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data\"\n",
    "             \"/master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "             \"22-13_01_25_169/CrossValidation/KFold_1/trainset/{f}\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data/\"\n",
    "            \"master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "            \"22-13_01_25_169/CrossValidation/KFold_1/testset/csv/{f}\"\n",
    "}\n",
    "\n",
    "LIST_OF_FILES = (\n",
    "    'alarm_query.csv\\nalarm_remove.csv\\nalarm_set.csv\\naudio_volum'\n",
    "    'e_down.csv\\naudio_volume_mute.csv\\naudio_volume_up.csv\\ncalend'\n",
    "    'ar_query.csv\\t\\ncalendar_remove.csv\\t\\ncalendar_set.csv\\t\\ncoo'\n",
    "    'king_recipe.csv\\t\\ndatetime_convert.csv\\t\\ndatetime_query.csv'\n",
    "    '\\t\\nemail_addcontact.csv\\t\\nemail_query.csv\\t\\nemail_querycon'\n",
    "    'tact.csv\\t\\nemail_sendemail.csv\\t\\ngeneral_affirm.csv\\t\\ngener'\n",
    "    'al_commandstop.csv\\t\\ngeneral_confirm.csv\\t\\ngeneral_dontcare.'\n",
    "    'csv\\t\\ngeneral_explain.csv\\t\\ngeneral_joke.csv\\t\\ngeneral_neg'\n",
    "    'ate.csv\\t\\ngeneral_praise.csv\\t\\ngeneral_quirky.csv\\t\\ngenera'\n",
    "    'l_repeat.csv\\t\\niot_cleaning.csv\\t\\niot_coffee.csv\\t\\niot_hue'\n",
    "    '_lightchange.csv\\t\\niot_hue_lightdim.csv\\t\\niot_hue_lightoff.'\n",
    "    'csv\\t\\niot_hue_lighton.csv\\t\\niot_hue_lightup.csv\\t\\niot_wemo_'\n",
    "    'off.csv\\t\\niot_wemo_on.csv\\t\\nlists_createoradd.csv\\t\\nlists_'\n",
    "    'query.csv\\t\\nlists_remove.csv\\t\\nmusic_likeness.csv\\t\\nmusic_q'\n",
    "    'uery.csv\\t\\nmusic_settings.csv\\t\\nnews_query.csv\\t\\nplay_audio'\n",
    "    'book.csv\\t\\nplay_game.csv\\t\\nplay_music.csv\\t\\nplay_podcasts.'\n",
    "    'csv\\t\\nplay_radio.csv\\t\\nqa_currency.csv\\t\\nqa_definition.csv'\n",
    "    '\\t\\nqa_factoid.csv\\t\\nqa_maths.csv\\t\\nqa_stock.csv\\t\\nrecomme'\n",
    "    'ndation_events.csv\\t\\nrecommendation_locations.csv\\t\\nrecomme'\n",
    "    'ndation_movies.csv\\t\\nsocial_post.csv\\t\\nsocial_query.csv\\t\\n'\n",
    "    'takeaway_order.csv\\t\\ntakeaway_query.csv\\t\\ntransport_query.c'\n",
    "    'sv\\t\\ntransport_taxi.csv\\t\\ntransport_ticket.csv\\t\\ntransport'\n",
    "    '_traffic.csv\\t\\nweather_query.csv\\t'.split())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_category_rows(fname: str, set_name: str):\n",
    "    pattern = PATTERNS[set_name]\n",
    "    url = pattern.format(f=fname)\n",
    "    request = requests.get(url)\n",
    "\n",
    "    reader = csv.reader(\n",
    "        io.StringIO(request.content.decode(\"utf-8\")), delimiter=\";\"\n",
    "    )\n",
    "    first_row = next(reader)\n",
    "    scenario_i, intent_i = first_row.index(\"scenario\"), first_row.index(\n",
    "        \"intent\")\n",
    "    answer_i = first_row.index(\"answer_from_anno\")\n",
    "\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        text = row[answer_i]\n",
    "        category = f\"{row[scenario_i]}_{row[intent_i]}\"\n",
    "        rows.append([text, category])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _get_final_rows(set_name: str):\n",
    "    final_rows = [_HEADER]\n",
    "    for f in tqdm(LIST_OF_FILES):\n",
    "        final_rows += _get_category_rows(f, set_name)\n",
    "    return final_rows\n",
    "\n",
    "\n",
    "def _write_data_into_file(path, rows):\n",
    "    with open(path, \"w\") as data_file:\n",
    "        writer = csv.writer(data_file, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def _main():\n",
    "    data_dir = os.getcwd()\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    print(\"Getting train data\")\n",
    "    train_rows = _get_final_rows(set_name=\"train\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"train.csv\"),\n",
    "        rows=train_rows\n",
    "    )\n",
    "\n",
    "    print(\"Getting test data\")\n",
    "    test_rows = _get_final_rows(set_name=\"test\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"test.csv\"),\n",
    "        rows=test_rows\n",
    "    )\n",
    "\n",
    "    print(\"Creating categories.json file\")\n",
    "    _, train_cats = zip(*train_rows[1:])\n",
    "    _, test_cats = zip(*test_rows[1:])\n",
    "    categories = sorted(list(\n",
    "        set(train_cats) | set(test_cats)\n",
    "    ))\n",
    "    with open(os.path.join(data_dir, \"categories.json\"), \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _main()\n"
   ],
   "id": "b3dac17b-c1d9-41f7-bb78-5608df62801b"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ],
   "id": "0b5085b1-99e4-428e-b7f9-314a42857750"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "train_data.columns = [\"speech_text\",\"intent\"]\n",
    "test_data.columns = [\"speech_text\",\"intent\"]"
   ],
   "id": "611d0548-e53f-4c25-a845-322d90c66115"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ],
   "id": "f383188b-db87-486f-baa8-3cf7fd604571"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = train_data['intent'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "intent_counts.plot(kind='bar')\n",
    "plt.title('Intent Counts')\n",
    "plt.xlabel('Intent')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "6bfd0fcb-b6b5-4676-b9fb-4911cc3dc091"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ],
   "id": "00da4924-d77b-420a-9712-f5d8f29033bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ],
   "id": "5399e9a0-116c-4519-b885-dcd43439c4a5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to generate random integers"
   ],
   "id": "7b628a8f-6f9e-4c42-a230-796ce6c49201"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_random_integer(n):\n",
    "    numbers = list(range(n))\n",
    "    random.shuffle(numbers)\n",
    "    for number in numbers:\n",
    "        yield number"
   ],
   "id": "29fcfe70-6469-441b-a1bd-c80f4d999ba7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function to implement the augmentation strategy"
   ],
   "id": "b7ab1e48-4efc-4d94-82f7-e6b85bfc0cd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(sentence):\n",
    "  alpha = 0.5\n",
    "  sentence = sentence.split(\" \")\n",
    "  n = int(alpha*len(sentence))\n",
    "  random_generator = generate_random_integer(len(sentence))\n",
    "  random_n = []\n",
    "  for _ in range(len(sentence)):\n",
    "    random_number = next(random_generator)\n",
    "    if sentence[random_number].lower() not in stop_words:\n",
    "      random_n.append(random_number)\n",
    "      if len(random_n) == n:\n",
    "        break\n",
    "  for num in random_n:\n",
    "    word = sentence[num]\n",
    "    synonyms = []\n",
    "    for synset in wordnet.synsets(word):\n",
    "      for synonym in synset.lemmas():\n",
    "        synonyms.append(synonym.name())\n",
    "    if len(synonyms)>=2:\n",
    "      sentence[num] = synonyms[1]\n",
    "    else:\n",
    "      pass\n",
    "  return \" \".join(sentence)"
   ],
   "id": "38f6f219-83de-4f4e-9581-f08fa6933eec"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = train_data.copy()\n",
    "augmented_data[\"intent\"] = augmented_data[\"intent\"].apply(augmentation)\n",
    "train_data = pd.concat([train_data, augmented_data], axis=0)\n"
   ],
   "id": "0d0557fa-6c6d-483c-a64c-3b26f2c7c3d5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting value counts post data augmentation"
   ],
   "id": "024c48f8-5a27-4619-8656-d6776be47841"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = train_data['intent'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "intent_counts.plot(kind='bar')\n",
    "plt.title('Intent Counts')\n",
    "plt.xlabel('Intent')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ],
   "id": "872ca609-e542-4be1-8528-efdd29dfb95c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data into train and validation"
   ],
   "id": "0684010e-9170-4e35-a594-02b031c3fe4d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val=train_test_split(train_data,test_size=0.10 ,random_state=seed)"
   ],
   "id": "6bbd6f1a-dca1-42ef-8fdd-c109067ab43a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting 60 sample of each intent"
   ],
   "id": "30052263-879b-4ad2-b3bb-59a75f67ce8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = df_train['intent'].unique()\n",
    "sampled_df = pd.DataFrame()\n",
    "for label in unique_labels:\n",
    "    label_df = df_train[df_train['intent'] == label]\n",
    "    samples = label_df.sample(n=60, random_state=seed)\n",
    "    sampled_df = sampled_df.append(samples)\n",
    "sampled_df.reset_index(drop=True, inplace=True)"
   ],
   "id": "ae063cf2-b51a-4ae1-9943-7b2e75d125e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating different dataset of 10 samples each with different data points."
   ],
   "id": "e44bd694-f63e-4f9d-b8f8-be942fce7ade"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sampled_df\n",
    "df['sampled'] = False\n",
    "\n",
    "label_counts = df['intent'].value_counts()\n",
    "\n",
    "max_count = label_counts.max()\n",
    "min_count = label_counts.min()\n",
    "\n",
    "num_datasets = max_count // 10\n",
    "\n",
    "training_datasets = []\n",
    "\n",
    "for i in range(num_datasets):\n",
    "    dataset = pd.DataFrame()\n",
    "    for label in df['intent'].unique():\n",
    "        label_df = df[(df['intent'] == label) & (df['sampled'] == False)]\n",
    "        if len(label_df) >= 10:\n",
    "            samples = label_df.sample(n=10)\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "        else:\n",
    "            samples = label_df\n",
    "            df.loc[samples.index, 'sampled'] = True\n",
    "            dataset = pd.concat([dataset, samples])\n",
    "    training_datasets.append(dataset)\n",
    "val_data = df_val"
   ],
   "id": "6b7b709b-7291-413c-a639-acdfdec61d29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ],
   "id": "1a0d63d7-4f63-4d05-afca-5dcf964af614"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ],
   "id": "a19d8969-5bf2-4ff0-bbb3-0a3e2f2d47a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "for train_data in training_datasets:\n",
    "  train_data['intent']=le.fit_transform(train_data['intent'])\n",
    "val_data['intent']=le.fit_transform(val_data['intent'])\n",
    "test_data['intent']=le.transform(test_data['intent'])"
   ],
   "id": "85a53e3a-fd31-419e-b681-4127549ca80e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up BERT Tokenizer and data loader"
   ],
   "id": "30332f63-47e9-4e34-84e0-b5a1a503f0df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_BERTmodel='bert-large-uncased'\n",
    "BERT_tokenizer=AutoTokenizer.from_pretrained(pre_trained_BERTmodel)\n"
   ],
   "id": "b97d9314-de07-4f40-b509-7e8473fede9d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(example):\n",
    "    encoded_input = BERT_tokenizer(example[\"speech_text\"], padding=\"max_length\", truncation=True)\n",
    "    return {\"input_ids\": encoded_input[\"input_ids\"], \"attention_mask\": encoded_input[\"attention_mask\"], \"labels\": example[\"intent\"]}"
   ],
   "id": "6b56fc18-c18c-4764-af46-b73d851b5b92"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=[]\n",
    "for train_data_ in training_datasets:\n",
    "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
    "  train_dataset.append(traindataset.map(tokenize_data))\n",
    "\n",
    "testdataset = datasets.Dataset.from_pandas(test_data)\n",
    "test_dataset = testdataset.map(tokenize_data)\n",
    "\n",
    "valdataset = datasets.Dataset.from_pandas(val_data)\n",
    "eval_dataset = valdataset.map(tokenize_data)"
   ],
   "id": "05ae4869-eed2-42f4-92de-b065cd2476e9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=[]\n",
    "for train_data_ in training_datasets:\n",
    "  traindataset = datasets.Dataset.from_pandas(train_data_)\n",
    "  train_dataset.append(traindataset.map(tokenize_data))\n",
    "\n",
    "testdataset = datasets.Dataset.from_pandas(test_data)\n",
    "test_dataset = testdataset.map(tokenize_data)\n",
    "\n",
    "valdataset = datasets.Dataset.from_pandas(val_data)\n",
    "eval_dataset = valdataset.map(tokenize_data)"
   ],
   "id": "1e1ef7fd-2d0c-4c08-b0c8-e7fc7b4ae4b1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up trainer arguments"
   ],
   "id": "836ab3cc-e36f-45ec-9001-0c096fc8cd25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=3e-5,\n",
    "        per_device_train_batch_size=8 ,\n",
    "        per_device_eval_batch_size=8 ,\n",
    "        num_train_epochs=20,\n",
    "        warmup_ratio= 0.1,\n",
    "        weight_decay= 0.001,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        save_total_limit=1,\n",
    "            )"
   ],
   "id": "c6cb5f86-d7c1-4d54-9858-351f3d94d4f8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Bert classifier"
   ],
   "id": "cb58c7a4-1419-4e35-a7f0-fe4e94eee741"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModelWithCustomLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertModelWithCustomLossFunction, self).__init__()\n",
    "        self.num_labels = len(df_train[\"intent\"].unique())\n",
    "        self.bert = BertModel.from_pretrained(\n",
    "            pre_trained_BERTmodel, num_labels=self.num_labels\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(1024, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        output = self.dropout(outputs.pooler_output)\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # you can define any loss function here yourself\n",
    "            # see https://pytorch.org/docs/stable/nn.html#loss-functions for an overview\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            # next, compute the loss based on logits + ground-truth labels\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ],
   "id": "3016f074-2a5a-4cc1-9ac1-1c05c114c850"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up compute metrics"
   ],
   "id": "8eaeb57b-9691-42ab-a8a4-8a9dae7f8d98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ],
   "id": "b810a4d2-4463-42f0-946f-35944685cbca"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ],
   "id": "3a6bf850-c90d-45fc-ba58-6d62ac2ae787"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy = 0\n",
    "for train_dat in train_dataset:\n",
    "  BERT_model = BertModelWithCustomLossFunction()\n",
    "  trainer = Trainer(\n",
    "        model = BERT_model,\n",
    "        args = args,\n",
    "        train_dataset=train_dat,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=BERT_tokenizer,\n",
    "        compute_metrics=compute_metrics,)\n",
    "  trainer.train()\n",
    "  evaluation_metrics = trainer.predict(test_dataset)\n",
    "  accuracy = evaluation_metrics.metrics['test_accuracy']\n",
    "  best_accuracy = max(accuracy, best_accuracy)\n",
    "  print(f\"Best Test Accuracy for this training dataset: {accuracy}\")\n",
    "  torch.cuda.empty_cache()"
   ],
   "id": "26f2587d-6df9-48cb-99c0-84d1596ce12c"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
