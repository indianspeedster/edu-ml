{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install transformers"
   ],
   "id": "fb9d9adc-afa9-4489-a18e-a36c08ec890f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "id": "743e7b24-db95-4976-b0a9-5695a3e697db"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevent libraries"
   ],
   "id": "20d52a1b-7ce3-4f3a-aa99-7749f5d503fb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AdamW, AdamWeightDecay, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F \n",
    "seed = 1331"
   ],
   "id": "c1621c6b-df25-418e-998a-7be51ae1d523"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "4cd5e072-18e1-456e-90fb-7251e581b1d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and store the data"
   ],
   "id": "481a5d58-c3e5-4caa-9ef7-fe1768a9cf16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_HEADER = [\"text\", \"category\"]\n",
    "PATTERNS = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data\"\n",
    "             \"/master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "             \"22-13_01_25_169/CrossValidation/KFold_1/trainset/{f}\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data/\"\n",
    "            \"master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "            \"22-13_01_25_169/CrossValidation/KFold_1/testset/csv/{f}\"\n",
    "}\n",
    "\n",
    "LIST_OF_FILES = (\n",
    "    'alarm_query.csv\\nalarm_remove.csv\\nalarm_set.csv\\naudio_volum'\n",
    "    'e_down.csv\\naudio_volume_mute.csv\\naudio_volume_up.csv\\ncalend'\n",
    "    'ar_query.csv\\t\\ncalendar_remove.csv\\t\\ncalendar_set.csv\\t\\ncoo'\n",
    "    'king_recipe.csv\\t\\ndatetime_convert.csv\\t\\ndatetime_query.csv'\n",
    "    '\\t\\nemail_addcontact.csv\\t\\nemail_query.csv\\t\\nemail_querycon'\n",
    "    'tact.csv\\t\\nemail_sendemail.csv\\t\\ngeneral_affirm.csv\\t\\ngener'\n",
    "    'al_commandstop.csv\\t\\ngeneral_confirm.csv\\t\\ngeneral_dontcare.'\n",
    "    'csv\\t\\ngeneral_explain.csv\\t\\ngeneral_joke.csv\\t\\ngeneral_neg'\n",
    "    'ate.csv\\t\\ngeneral_praise.csv\\t\\ngeneral_quirky.csv\\t\\ngenera'\n",
    "    'l_repeat.csv\\t\\niot_cleaning.csv\\t\\niot_coffee.csv\\t\\niot_hue'\n",
    "    '_lightchange.csv\\t\\niot_hue_lightdim.csv\\t\\niot_hue_lightoff.'\n",
    "    'csv\\t\\niot_hue_lighton.csv\\t\\niot_hue_lightup.csv\\t\\niot_wemo_'\n",
    "    'off.csv\\t\\niot_wemo_on.csv\\t\\nlists_createoradd.csv\\t\\nlists_'\n",
    "    'query.csv\\t\\nlists_remove.csv\\t\\nmusic_likeness.csv\\t\\nmusic_q'\n",
    "    'uery.csv\\t\\nmusic_settings.csv\\t\\nnews_query.csv\\t\\nplay_audio'\n",
    "    'book.csv\\t\\nplay_game.csv\\t\\nplay_music.csv\\t\\nplay_podcasts.'\n",
    "    'csv\\t\\nplay_radio.csv\\t\\nqa_currency.csv\\t\\nqa_definition.csv'\n",
    "    '\\t\\nqa_factoid.csv\\t\\nqa_maths.csv\\t\\nqa_stock.csv\\t\\nrecomme'\n",
    "    'ndation_events.csv\\t\\nrecommendation_locations.csv\\t\\nrecomme'\n",
    "    'ndation_movies.csv\\t\\nsocial_post.csv\\t\\nsocial_query.csv\\t\\n'\n",
    "    'takeaway_order.csv\\t\\ntakeaway_query.csv\\t\\ntransport_query.c'\n",
    "    'sv\\t\\ntransport_taxi.csv\\t\\ntransport_ticket.csv\\t\\ntransport'\n",
    "    '_traffic.csv\\t\\nweather_query.csv\\t'.split())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_category_rows(fname: str, set_name: str):\n",
    "    pattern = PATTERNS[set_name]\n",
    "    url = pattern.format(f=fname)\n",
    "    request = requests.get(url)\n",
    "\n",
    "    reader = csv.reader(\n",
    "        io.StringIO(request.content.decode(\"utf-8\")), delimiter=\";\"\n",
    "    )\n",
    "    first_row = next(reader)\n",
    "    scenario_i, intent_i = first_row.index(\"scenario\"), first_row.index(\n",
    "        \"intent\")\n",
    "    answer_i = first_row.index(\"answer_from_anno\")\n",
    "\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        text = row[answer_i]\n",
    "        category = f\"{row[scenario_i]}_{row[intent_i]}\"\n",
    "        rows.append([text, category])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _get_final_rows(set_name: str):\n",
    "    final_rows = [_HEADER]\n",
    "    for f in tqdm(LIST_OF_FILES):\n",
    "        final_rows += _get_category_rows(f, set_name)\n",
    "    return final_rows\n",
    "\n",
    "\n",
    "def _write_data_into_file(path, rows):\n",
    "    with open(path, \"w\") as data_file:\n",
    "        writer = csv.writer(data_file, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def _main():\n",
    "    data_dir = os.getcwd()\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    print(\"Getting train data\")\n",
    "    train_rows = _get_final_rows(set_name=\"train\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"train.csv\"),\n",
    "        rows=train_rows\n",
    "    )\n",
    "\n",
    "    print(\"Getting test data\")\n",
    "    test_rows = _get_final_rows(set_name=\"test\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"test.csv\"),\n",
    "        rows=test_rows\n",
    "    )\n",
    "\n",
    "    print(\"Creating categories.json file\")\n",
    "    _, train_cats = zip(*train_rows[1:])\n",
    "    _, test_cats = zip(*test_rows[1:])\n",
    "    categories = sorted(list(\n",
    "        set(train_cats) | set(test_cats)\n",
    "    ))\n",
    "    with open(os.path.join(data_dir, \"categories.json\"), \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _main()\n"
   ],
   "id": "1485bc7e-e9e0-4023-80db-a7951fde8578"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ],
   "id": "8c76d8ff-b34d-466b-b7f9-9e6a9f193197"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "train_data.columns = [\"speech_text\",\"intent\"]\n",
    "test_data.columns = [\"speech_text\",\"intent\"]"
   ],
   "id": "2b5b861e-4f5b-46f5-941c-f3a219a63b90"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ],
   "id": "01e90351-2947-4bb7-98ae-858ce553c98c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = train_data['intent'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "intent_counts.plot(kind='bar')\n",
    "plt.title('Intent Counts')\n",
    "plt.xlabel('Intent')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "36c106fc-e2ab-4a1a-8ea9-56e02f2a0149"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ],
   "id": "3a564b02-b2ed-4774-b816-a4abf5a4aabf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "train_data['intent']=le.fit_transform(train_data['intent'])\n",
    "test_data['intent']=le.transform(test_data['intent'])\n",
    "print(le.classes_)"
   ],
   "id": "a3e5e628-3b0d-42d5-8d64-828107df8f7c"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up BERT Tokenizer"
   ],
   "id": "72a4532f-5b83-46ff-8f4e-ceb170de72b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_BERTmodel='bert-large-uncased'\n",
    "BERT_tokenizer=BertTokenizerFast.from_pretrained(pre_trained_BERTmodel)"
   ],
   "id": "8de5878c-32f2-4872-8886-2bde5b9fe9f9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch dataset class\n",
    "\n",
    "This dataset class will be used to serve data to Dataloader"
   ],
   "id": "caad00fd-ad25-4e3f-b32a-4543f6513367"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_text_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self,text,intent,tokenizer,max_length):\n",
    "    self.text=text\n",
    "    self.intent=intent\n",
    "    self.tokenizer=tokenizer\n",
    "    self.max_length=max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self,item):\n",
    "    text = str(self.text[item])\n",
    "    intent = self.intent[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=Max_length,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "       )\n",
    "           \n",
    "    return {\n",
    "        'speech_text':text,\n",
    "        'input_ids':encoding['input_ids'].flatten(),\n",
    "        'attention_mask':encoding['attention_mask'].flatten(),\n",
    "        'intent' : torch.tensor(intent,dtype=torch.long)\n",
    "    }"
   ],
   "id": "5d2e1f61-0eff-477c-89e0-36bd33c55e93"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data into training and validation"
   ],
   "id": "5077539d-3f6d-4343-a1d3-f16995d39284"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val=train_test_split(train_data,test_size=0.15 ,random_state=seed)\n",
    "\n",
    "print('Print the shape of datasets...')\n",
    "print(f'Training dataset : {df_train.shape} ')\n",
    "print(f'Testing dataset : {test_data.shape}') \n",
    "print(f'Validation dataset : {df_val.shape}')"
   ],
   "id": "de6acbe5-d005-49d8-8a24-72bfb7293747"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataloader\n",
    "\n",
    "This will be used to train model and make sure that the data is being fed in batches"
   ],
   "id": "dcd737cd-42f4-40db-983e-94ac9530dfe3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "Max_length= 50\n",
    "def data_loader(df,tokenizer, max_length, batch):\n",
    "  ds=Speech_text_Dataset(\n",
    "      text=df.speech_text.to_numpy(),\n",
    "      intent=df.intent.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_length=Max_length\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=4\n",
    "  )\n",
    "\n",
    "train_DataLoader=data_loader(df_train, BERT_tokenizer,Max_length,batch_size)\n",
    "test_DataLoader=data_loader(test_data, BERT_tokenizer,Max_length,batch_size)\n",
    "valid_DataLoader=data_loader(df_val, BERT_tokenizer,Max_length,batch_size)"
   ],
   "id": "ddf5546a-b512-429d-a4de-cfc6e6f106b6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_data=next(iter(train_DataLoader))\n",
    "input_ids = BERT_data['input_ids'].to(device)\n",
    "attention_mask = BERT_data['attention_mask'].to(device)\n",
    "targets=BERT_data['intent'].to(device)\n",
    "print('Shape of the BERT_data keys...')\n",
    "print(f\"Input_ids : {BERT_data['input_ids'].shape}\")\n",
    "print(f\"Attention_mask : {BERT_data['attention_mask'].shape}\")\n",
    "print(f\"targets : {BERT_data['intent'].shape}\")"
   ],
   "id": "7c7ccbaf-a8ac-406e-aa25-0a1405225793"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained BERT Model\n",
    "\n",
    "This Bert model will be used for training with slight modification ie, adding an extra layer at the end"
   ],
   "id": "912a5328-7148-4d4d-97f1-70e91dd74275"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model = BertModel.from_pretrained(pre_trained_BERTmodel)\n",
    "BERT_model=BERT_model.to(device)"
   ],
   "id": "c8065490-8e85-4243-ad55-3af360e7c55e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes= len(train_data[\"intent\"].unique())"
   ],
   "id": "fd5648a5-060b-45cd-b1ee-bcdadf667239"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_IntentClassifier(nn.Module):\n",
    "   def __init__(self, n_classes):\n",
    "     super(BERT_IntentClassifier, self).__init__()\n",
    "     self.bert = BertModel.from_pretrained(pre_trained_BERTmodel)\n",
    "     self.drop = nn.Dropout(p=0.1)\n",
    "     self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "   def forward(self, input_ids, attention_mask):\n",
    "     _, pooled_output = self.bert(\n",
    "         input_ids=input_ids,\n",
    "         attention_mask=attention_mask,return_dict=False\n",
    "    )\n",
    "     output = self.drop(pooled_output)\n",
    "     output=self.out(output)\n",
    "     return output"
   ],
   "id": "836c47b7-4385-4b63-91a5-d243521e24e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model = BERT_IntentClassifier(n_classes)\n",
    "BERT_model=BERT_model.to(device)"
   ],
   "id": "98e7b533-da55-4754-94ef-ab907b2c57ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(BERT_model(input_ids,attention_mask), dim=1).to(device)"
   ],
   "id": "2da51ada-7310-4398-af15-432fe64d3d30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Training hyperparameters"
   ],
   "id": "59ec5b99-8f5b-43a7-bfee-3a06756fbaf9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=6\n",
    "optimizer=AdamW(BERT_model.parameters(),lr=4e-5)\n",
    "total_steps=len(train_DataLoader)*epochs\n",
    "\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss().to(device)"
   ],
   "id": "0d23d097-044a-4ca8-b4cb-cdca21cbdb37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval Function"
   ],
   "id": "3c70108c-7df5-41d0-b091-ee63a0640402"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train(\n",
    "  model,\n",
    "  data_loader,\n",
    "  loss_fn,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_observations\n",
    "):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"intent\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "      )\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()   \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_observations, np.mean(losses)"
   ],
   "id": "79f19784-38db-407d-99c3-d817b9fb7722"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader,device,loss_fn, n_observations):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"intent\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_observations, np.mean(losses)"
   ],
   "id": "7c2a21e4-5408-4a7b-a865-fa56c98199c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ],
   "id": "b89b489d-591f-4e46-ad6f-f7e0397939c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "  print(f'Epoch {epoch + 1}/{epochs}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train(\n",
    "    BERT_model,\n",
    "    train_DataLoader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(df_train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    BERT_model,\n",
    "    valid_DataLoader,\n",
    "    device,\n",
    "    loss_fn,\n",
    "    len(df_val)\n",
    "  )\n",
    "  print(f'Validation  loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(BERT_model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ],
   "id": "fa6ec626-78cf-4f8e-86be-fb1d884b67e5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "39d41fa5-34af-4534-bf58-a1479dac87bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "3f1f014e-e6db-49b2-8a0e-ba6e14d7eda4"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
