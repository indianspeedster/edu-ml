{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install transformers"
   ],
   "id": "04a82b0a-09b5-4907-b3dd-917721fcb6ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ],
   "id": "18601977-f800-4d99-b317-9b2fbbe8bf48"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevent libraries"
   ],
   "id": "5a2b086a-eaab-4511-ad6d-b7c4f6a029c7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizerFast\n",
    "from transformers import AdamW, AdamWeightDecay, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F \n",
    "seed = 1331"
   ],
   "id": "59d41a3b-59c4-498a-b80e-51e70d7a3f8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda:0')\n",
    "torch.cuda.get_device_name(0)"
   ],
   "id": "576477b9-4ca4-4a1a-b145-8ea40429e3b5"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and store the data"
   ],
   "id": "1c087f8b-671e-45c4-ab25-ebd1b8890ccc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_HEADER = [\"text\", \"category\"]\n",
    "PATTERNS = {\n",
    "    \"train\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data\"\n",
    "             \"/master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "             \"22-13_01_25_169/CrossValidation/KFold_1/trainset/{f}\",\n",
    "    \"test\": \"https://raw.githubusercontent.com/xliuhw/NLU-Evaluation-Data/\"\n",
    "            \"master/CrossValidation/autoGeneFromRealAnno/autoGene_2018_03_\"\n",
    "            \"22-13_01_25_169/CrossValidation/KFold_1/testset/csv/{f}\"\n",
    "}\n",
    "\n",
    "LIST_OF_FILES = (\n",
    "    'alarm_query.csv\\nalarm_remove.csv\\nalarm_set.csv\\naudio_volum'\n",
    "    'e_down.csv\\naudio_volume_mute.csv\\naudio_volume_up.csv\\ncalend'\n",
    "    'ar_query.csv\\t\\ncalendar_remove.csv\\t\\ncalendar_set.csv\\t\\ncoo'\n",
    "    'king_recipe.csv\\t\\ndatetime_convert.csv\\t\\ndatetime_query.csv'\n",
    "    '\\t\\nemail_addcontact.csv\\t\\nemail_query.csv\\t\\nemail_querycon'\n",
    "    'tact.csv\\t\\nemail_sendemail.csv\\t\\ngeneral_affirm.csv\\t\\ngener'\n",
    "    'al_commandstop.csv\\t\\ngeneral_confirm.csv\\t\\ngeneral_dontcare.'\n",
    "    'csv\\t\\ngeneral_explain.csv\\t\\ngeneral_joke.csv\\t\\ngeneral_neg'\n",
    "    'ate.csv\\t\\ngeneral_praise.csv\\t\\ngeneral_quirky.csv\\t\\ngenera'\n",
    "    'l_repeat.csv\\t\\niot_cleaning.csv\\t\\niot_coffee.csv\\t\\niot_hue'\n",
    "    '_lightchange.csv\\t\\niot_hue_lightdim.csv\\t\\niot_hue_lightoff.'\n",
    "    'csv\\t\\niot_hue_lighton.csv\\t\\niot_hue_lightup.csv\\t\\niot_wemo_'\n",
    "    'off.csv\\t\\niot_wemo_on.csv\\t\\nlists_createoradd.csv\\t\\nlists_'\n",
    "    'query.csv\\t\\nlists_remove.csv\\t\\nmusic_likeness.csv\\t\\nmusic_q'\n",
    "    'uery.csv\\t\\nmusic_settings.csv\\t\\nnews_query.csv\\t\\nplay_audio'\n",
    "    'book.csv\\t\\nplay_game.csv\\t\\nplay_music.csv\\t\\nplay_podcasts.'\n",
    "    'csv\\t\\nplay_radio.csv\\t\\nqa_currency.csv\\t\\nqa_definition.csv'\n",
    "    '\\t\\nqa_factoid.csv\\t\\nqa_maths.csv\\t\\nqa_stock.csv\\t\\nrecomme'\n",
    "    'ndation_events.csv\\t\\nrecommendation_locations.csv\\t\\nrecomme'\n",
    "    'ndation_movies.csv\\t\\nsocial_post.csv\\t\\nsocial_query.csv\\t\\n'\n",
    "    'takeaway_order.csv\\t\\ntakeaway_query.csv\\t\\ntransport_query.c'\n",
    "    'sv\\t\\ntransport_taxi.csv\\t\\ntransport_ticket.csv\\t\\ntransport'\n",
    "    '_traffic.csv\\t\\nweather_query.csv\\t'.split())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _get_category_rows(fname: str, set_name: str):\n",
    "    pattern = PATTERNS[set_name]\n",
    "    url = pattern.format(f=fname)\n",
    "    request = requests.get(url)\n",
    "\n",
    "    reader = csv.reader(\n",
    "        io.StringIO(request.content.decode(\"utf-8\")), delimiter=\";\"\n",
    "    )\n",
    "    first_row = next(reader)\n",
    "    scenario_i, intent_i = first_row.index(\"scenario\"), first_row.index(\n",
    "        \"intent\")\n",
    "    answer_i = first_row.index(\"answer_from_anno\")\n",
    "\n",
    "    rows = []\n",
    "    for row in reader:\n",
    "        text = row[answer_i]\n",
    "        category = f\"{row[scenario_i]}_{row[intent_i]}\"\n",
    "        rows.append([text, category])\n",
    "    return rows\n",
    "\n",
    "\n",
    "def _get_final_rows(set_name: str):\n",
    "    final_rows = [_HEADER]\n",
    "    for f in tqdm(LIST_OF_FILES):\n",
    "        final_rows += _get_category_rows(f, set_name)\n",
    "    return final_rows\n",
    "\n",
    "\n",
    "def _write_data_into_file(path, rows):\n",
    "    with open(path, \"w\") as data_file:\n",
    "        writer = csv.writer(data_file, quoting=csv.QUOTE_ALL)\n",
    "        writer.writerows(rows)\n",
    "\n",
    "\n",
    "def _main():\n",
    "    data_dir = os.getcwd()\n",
    "\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.mkdir(data_dir)\n",
    "\n",
    "    print(\"Getting train data\")\n",
    "    train_rows = _get_final_rows(set_name=\"train\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"train.csv\"),\n",
    "        rows=train_rows\n",
    "    )\n",
    "\n",
    "    print(\"Getting test data\")\n",
    "    test_rows = _get_final_rows(set_name=\"test\")\n",
    "    _write_data_into_file(\n",
    "        path=os.path.join(data_dir, \"test.csv\"),\n",
    "        rows=test_rows\n",
    "    )\n",
    "\n",
    "    print(\"Creating categories.json file\")\n",
    "    _, train_cats = zip(*train_rows[1:])\n",
    "    _, test_cats = zip(*test_rows[1:])\n",
    "    categories = sorted(list(\n",
    "        set(train_cats) | set(test_cats)\n",
    "    ))\n",
    "    with open(os.path.join(data_dir, \"categories.json\"), \"w\") as f:\n",
    "        json.dump(categories, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _main()\n"
   ],
   "id": "e9ca3a67-e212-49b1-bc19-01afb61246d0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and visualize data"
   ],
   "id": "637bc8c6-e9d4-48a8-bf3a-6a7321c4db28"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "train_data.columns = [\"speech_text\",\"intent\"]\n",
    "test_data.columns = [\"speech_text\",\"intent\"]"
   ],
   "id": "8fc347da-3933-4b91-9344-b1848dfc4d4e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ],
   "id": "f80f2920-1d1a-4fb8-9f39-f9edae20666a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intent_counts = train_data['intent'].value_counts()\n",
    "plt.figure(figsize=(10, 6))\n",
    "intent_counts.plot(kind='bar')\n",
    "plt.title('Intent Counts')\n",
    "plt.xlabel('Intent')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ],
   "id": "de3f3c2f-19fb-4a7a-a056-ee7706d101af"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode the labels"
   ],
   "id": "422056de-70a6-4664-9325-fd8ba9025015"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le=LabelEncoder()\n",
    "train_data['intent']=le.fit_transform(train_data['intent'])\n",
    "test_data['intent']=le.transform(test_data['intent'])\n",
    "print(le.classes_)"
   ],
   "id": "867745c1-aeae-4868-b922-c9bc64c85e2a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up BERT Tokenizer"
   ],
   "id": "b83fe05a-9f18-4687-969d-7851bc9add4b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_BERTmodel='bert-large-uncased'\n",
    "BERT_tokenizer=BertTokenizerFast.from_pretrained(pre_trained_BERTmodel)"
   ],
   "id": "4ed1dabf-7527-425a-a092-c3b15a631c68"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create torch dataset class\n",
    "\n",
    "This dataset class will be used to serve data to Dataloader"
   ],
   "id": "8b61d00d-1a77-404c-bf8e-071e96cd2666"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Speech_text_Dataset(Dataset):\n",
    "\n",
    "  def __init__(self,text,intent,tokenizer,max_length):\n",
    "    self.text=text\n",
    "    self.intent=intent\n",
    "    self.tokenizer=tokenizer\n",
    "    self.max_length=max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.text)\n",
    "\n",
    "  def __getitem__(self,item):\n",
    "    text = str(self.text[item])\n",
    "    intent = self.intent[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        max_length=Max_length,\n",
    "        add_special_tokens=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_token_type_ids=False,\n",
    "        return_tensors='pt'\n",
    "       )\n",
    "           \n",
    "    return {\n",
    "        'speech_text':text,\n",
    "        'input_ids':encoding['input_ids'].flatten(),\n",
    "        'attention_mask':encoding['attention_mask'].flatten(),\n",
    "        'intent' : torch.tensor(intent,dtype=torch.long)\n",
    "    }"
   ],
   "id": "410fbb14-6855-44a0-bc8b-7c7a23691f7a"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the training data into training and validation"
   ],
   "id": "ad778241-80ae-4e7e-a9fb-33bafba294fa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_val=train_test_split(train_data,test_size=0.15 ,random_state=seed)\n",
    "\n",
    "print('Print the shape of datasets...')\n",
    "print(f'Training dataset : {df_train.shape} ')\n",
    "print(f'Testing dataset : {test_data.shape}') \n",
    "print(f'Validation dataset : {df_val.shape}')"
   ],
   "id": "90b63dc1-b17a-44f2-9774-bd3d82997c92"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Dataloader\n",
    "\n",
    "This will be used to train model and make sure that the data is being fed in batches"
   ],
   "id": "18dc0550-08ce-4e31-81b7-1314cf05296b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "Max_length= 50\n",
    "def data_loader(df,tokenizer, max_length, batch):\n",
    "  ds=Speech_text_Dataset(\n",
    "      text=df.speech_text.to_numpy(),\n",
    "      intent=df.intent.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_length=Max_length\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "      num_workers=4\n",
    "  )\n",
    "\n",
    "train_DataLoader=data_loader(df_train, BERT_tokenizer,Max_length,batch_size)\n",
    "test_DataLoader=data_loader(test_data, BERT_tokenizer,Max_length,batch_size)\n",
    "valid_DataLoader=data_loader(df_val, BERT_tokenizer,Max_length,batch_size)"
   ],
   "id": "e019da81-786e-4bdc-9953-f93f4109742a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_data=next(iter(train_DataLoader))\n",
    "input_ids = BERT_data['input_ids'].to(device)\n",
    "attention_mask = BERT_data['attention_mask'].to(device)\n",
    "targets=BERT_data['intent'].to(device)\n",
    "print('Shape of the BERT_data keys...')\n",
    "print(f\"Input_ids : {BERT_data['input_ids'].shape}\")\n",
    "print(f\"Attention_mask : {BERT_data['attention_mask'].shape}\")\n",
    "print(f\"targets : {BERT_data['intent'].shape}\")"
   ],
   "id": "fad35c52-41c6-4f12-a652-6421623391d6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pre-trained BERT Model\n",
    "\n",
    "This Bert model will be used for training with slight modification ie, adding an extra layer at the end"
   ],
   "id": "7dac905f-c16a-4f76-877f-6e230030dfd2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model = BertModel.from_pretrained(pre_trained_BERTmodel)\n",
    "BERT_model=BERT_model.to(device)"
   ],
   "id": "e4325317-592a-42a7-a0dd-a066cd62e9e6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes= len(train_data[\"intent\"].unique())"
   ],
   "id": "bd346c47-0ea1-4a2a-82aa-f25c50d19c78"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_IntentClassifier(nn.Module):\n",
    "   def __init__(self, n_classes):\n",
    "     super(BERT_IntentClassifier, self).__init__()\n",
    "     self.bert = BertModel.from_pretrained(pre_trained_BERTmodel)\n",
    "     self.drop = nn.Dropout(p=0.1)\n",
    "     self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "   def forward(self, input_ids, attention_mask):\n",
    "     _, pooled_output = self.bert(\n",
    "         input_ids=input_ids,\n",
    "         attention_mask=attention_mask,return_dict=False\n",
    "    )\n",
    "     output = self.drop(pooled_output)\n",
    "     output=self.out(output)\n",
    "     return output"
   ],
   "id": "385dd8f0-b0ec-433c-b85f-a3a5070c6c96"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_model = BERT_IntentClassifier(n_classes)\n",
    "BERT_model=BERT_model.to(device)"
   ],
   "id": "1ab1fcd7-06c2-4e7e-9115-72305003555a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax(BERT_model(input_ids,attention_mask), dim=1).to(device)"
   ],
   "id": "a9a649b2-2c24-4acb-aefe-fd2772737a5d"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the Training hyperparameters"
   ],
   "id": "5cb51348-2c39-4c0e-a4f2-1f24a37680e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=6\n",
    "optimizer=AdamW(BERT_model.parameters(),lr=4e-5)\n",
    "total_steps=len(train_DataLoader)*epochs\n",
    "\n",
    "scheduler=get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn=nn.CrossEntropyLoss().to(device)"
   ],
   "id": "3b9834ec-977e-44f1-ba58-972e7b06fa26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and eval Function"
   ],
   "id": "cfc24d4a-6e8c-41e3-998a-e979165d5786"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " train(\n",
    "  model,\n",
    "  data_loader,\n",
    "  loss_fn,\n",
    "  optimizer,\n",
    "  device,\n",
    "  scheduler,\n",
    "  n_observations\n",
    "):\n",
    "  model = model.train()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  for d in data_loader:\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"intent\"].to(device)\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "      )\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "    loss.backward()   \n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "  return correct_predictions.double() / n_observations, np.mean(losses)"
   ],
   "id": "37cae94a-e6fd-4a18-b39c-859b5a727a3c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader,device,loss_fn, n_observations):\n",
    "  model = model.eval()\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"intent\"].to(device)\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "      loss = loss_fn(outputs, targets)\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "  return correct_predictions.double() / n_observations, np.mean(losses)"
   ],
   "id": "2c581412-c257-4dc9-b74b-79b43fe873eb"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune the model"
   ],
   "id": "937d3e66-b24e-49e0-91d0-60c122549882"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "for epoch in range(epochs):\n",
    "  print(f'Epoch {epoch + 1}/{epochs}')\n",
    "  print('-' * 10)\n",
    "  train_acc, train_loss = train(\n",
    "    BERT_model,\n",
    "    train_DataLoader,\n",
    "    loss_fn,\n",
    "    optimizer,\n",
    "    device,\n",
    "    scheduler,\n",
    "    len(df_train)\n",
    "  )\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "  val_acc, val_loss = eval_model(\n",
    "    BERT_model,\n",
    "    valid_DataLoader,\n",
    "    device,\n",
    "    loss_fn,\n",
    "    len(df_val)\n",
    "  )\n",
    "  print(f'Validation  loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(BERT_model.state_dict(), 'best_model_state.bin')\n",
    "    best_accuracy = val_acc"
   ],
   "id": "556a80e5-a744-4f4b-a9ec-5074bd338824"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [],
   "id": "c7671db1-8ff9-402e-aa7f-e378c4d891d2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [],
   "id": "98e523ca-5927-4eba-88d0-c80e770ec2dd"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
